{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "system"
    ]
   },
   "source": [
    "> You are a code generator that can only answer with python code."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File: 'example.ts'\n",
    "\n",
    "```typescript\n",
    "function greet(name: string) {\n",
    "  console.log(\"Hello, \" + name);\n",
    "}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add the age of the user to the console log.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": [
     "assistant"
    ]
   },
   "outputs": [],
   "source": [
    "with open('example.ts', 'w') as f: f.write(\"\"\"\n",
    "function greet(name: string, age : number) {\n",
    "  console.log(`Hello, ${name}. You are ${age} years old.`);\n",
    "}\n",
    "\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File: '../../src/finishReason.ts'\n",
    "\n",
    "```typescript\n",
    "export enum FinishReason {\n",
    "  length,\n",
    "  contentFilter,\n",
    "  stop,\n",
    "  null,\n",
    "  cancelled\n",
    "}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**My compiler says that FinishReason \"timeout\" is missing. Fix the issue.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "assistant"
    ]
   },
   "outputs": [],
   "source": [
    "with open('../../src/finishReason.ts', 'w') as f: f.write(\"\"\"\n",
    "export enum FinishReason {\n",
    "  length,\n",
    "  contentFilter,\n",
    "  stop,\n",
    "  null,\n",
    "  cancelled,\n",
    "  timeout\n",
    "}\n",
    "\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "File: '../../src/completion.ts'\n",
      "\n",
      " \n",
      "import axios from \"axios\";\n",
      "import { ChatCompletionRequestMessage, Configuration, CreateChatCompletionRequest, OpenAIApi } from \"openai\";\n",
      "import {\n",
      "  CancellationToken,\n",
      "  ConfigurationTarget,\n",
      "  NotebookCellKind,\n",
      "  NotebookEdit,\n",
      "  NotebookRange,\n",
      "  QuickPickItem,\n",
      "  WorkspaceEdit,\n",
      "  window,\n",
      "  workspace,\n",
      "} from \"vscode\";\n",
      "import { appendTextToCell, convertCellsToMessages, insertCell } from \"./cellUtils\";\n",
      "import { CompletionType } from \"./completionType\";\n",
      "import { FinishReason } from \"./finishReason\";\n",
      "import { bufferWholeChunks, streamChatCompletion } from \"./streamUtils\";\n",
      "import { UIProgress } from \"./uiProgress\";\n",
      "import { encoding_for_model } from \"@dqbd/tiktoken\";\n",
      "import { configKeys, msgs, uiText } from \"./constants\";\n",
      "\n",
      "const output = window.createOutputChannel(\"Notebook ChatCompletion\");\n",
      "type TokenReductionStrategy = QuickPickItem & {\n",
      "  apply: Function;\n",
      "  savedTokens?: number;\n",
      "};\n",
      "\n",
      "export async function generateCompletion(\n",
      "  cellIndex: number,\n",
      "  completionType: CompletionType,\n",
      "  progress: UIProgress,\n",
      "  cancelToken: CancellationToken\n",
      "): Promise<FinishReason> {\n",
      "  const e = window.activeNotebookEditor!;\n",
      "  let messages = await convertCellsToMessages(cellIndex, completionType);\n",
      "  let ck: NotebookCellKind | undefined = undefined;\n",
      "\n",
      "  const openaiApiKey = await getOpenAIApiKey();\n",
      "\n",
      "  if (!openaiApiKey) {\n",
      "    throw new Error(msgs.apiKeyNotSet);\n",
      "  }\n",
      "\n",
      "  const openai = new OpenAIApi(new Configuration({ apiKey: openaiApiKey }));\n",
      "\n",
      "  const tokenSource = axios.CancelToken.source();\n",
      "  cancelToken.onCancellationRequested(tokenSource.cancel);\n",
      "\n",
      "  const nbMetadata = e.notebook.metadata.custom;\n",
      "\n",
      "  const defaultModel = workspace.getConfiguration().get<string>(\"notebook-chatcompletion.defaultModel\");\n",
      "\n",
      "  const model = nbMetadata?.model ?? defaultModel;\n",
      "  const temperature = nbMetadata?.temperature ?? 0;\n",
      "\n",
      "  const limit = getTokenLimit(model);\n",
      "\n",
      "  const msgText = JSON.stringify(messages);\n",
      "  const totalTokenCount = countTokens(msgText, model);\n",
      "\n",
      "  if (limit !== null && totalTokenCount > limit) {\n",
      "    const tokenOverflow = limit - totalTokenCount;\n",
      "\n",
      "    const msgText = messages.map((x) => x.content).join();\n",
      "    const contentTokenCount = countTokens(msgText, model);\n",
      "\n",
      "    const reducedMessages = await applyTokenReductionStrategies(messages, tokenOverflow, contentTokenCount, limit, model);\n",
      "\n",
      "    if (!reducedMessages) {\n",
      "      return FinishReason.cancelled;\n",
      "    }\n",
      "\n",
      "    messages = reducedMessages;\n",
      "  }\n",
      "\n",
      "  let reqParams: CreateChatCompletionRequest = {\n",
      "    model: model,\n",
      "    messages: messages,\n",
      "    stream: true,\n",
      "    temperature: temperature,\n",
      "  };\n",
      "\n",
      "  if (limit) {\n",
      "    const reducedMsgText = JSON.stringify(messages);\n",
      "    const reducedTokenCount = countTokens(reducedMsgText, model);\n",
      "    reqParams.max_tokens = limit - reducedTokenCount;\n",
      "\n",
      "    if (reqParams.max_tokens < 1) {\n",
      "      const result = await window.showInformationMessage(\n",
      "        `The request is estimated to be ${-reqParams.max_tokens} tokens over the limit (including the input) and will likely be rejected from the OpenAI API. Do you still want to proceed?`,\n",
      "        { modal: true },\n",
      "        \"Yes\"\n",
      "      );\n",
      "      if (result !== \"Yes\") {\n",
      "        return FinishReason.cancelled;\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "\n",
      "  reqParams = addParametersFromMetadata(nbMetadata, reqParams);\n",
      "\n",
      "  output.appendLine(\"\\n\" + JSON.stringify(reqParams, undefined, 2) + \"\\n\");\n",
      "  progress.report({ increment: 1, message: msgs.sendingRequest });\n",
      "\n",
      "  const response = await openai.createChatCompletion(reqParams, {\n",
      "    cancelToken: tokenSource.token,\n",
      "    responseType: \"stream\",\n",
      "  });\n",
      "\n",
      "  for await (let textToken of bufferWholeChunks(streamChatCompletion(response, cancelToken))) {\n",
      "    if (Object.values(FinishReason).includes(textToken as FinishReason)) {\n",
      "      switch (textToken) {\n",
      "        case FinishReason.length:\n",
      "          output.append(\"FINISH_REASON_LENGTH\" + \"\\n\");\n",
      "          break;\n",
      "        case FinishReason.contentFilter:\n",
      "          output.append(\"FINISH_REASON_CONTENTFILTER\" + \"\\n\");\n",
      "          break;\n",
      "        case FinishReason.stop:\n",
      "          output.append(\"FINISH_REASON_STOP\" + \"\\n\");\n",
      "          break;\n",
      "      }\n",
      "\n",
      "      const currentCell = e.notebook.cellAt(cellIndex);\n",
      "      const text = currentCell.document.getText();\n",
      "\n",
      "      if (!/\\S/.test(text)) {\n",
      "        const edit = new WorkspaceEdit();\n",
      "        edit.set(currentCell.notebook.uri, [NotebookEdit.deleteCells(new NotebookRange(currentCell.index, currentCell.index + 1))]);\n",
      "        await workspace.applyEdit(edit);\n",
      "      }\n",
      "\n",
      "      return textToken as FinishReason;\n",
      "    } else {\n",
      "      output.append(textToken.toString());\n",
      "    }\n",
      "\n",
      "    if (typeof textToken !== \"string\") {\n",
      "      throw new Error(`Unknown stream result: ${textToken}`);\n",
      "    }\n",
      "\n",
      "    if (textToken.includes(\"```python\\n\")) {\n",
      "      ck = NotebookCellKind.Code;\n",
      "\n",
      "      cellIndex = await insertCell(e, cellIndex, ck, \"python\");\n",
      "      textToken = textToken.replace(\"```python\\n\", \"\");\n",
      "    } else if (textToken.includes(\"```\") && ck === NotebookCellKind.Code) {\n",
      "      textToken = textToken.replace(\"```\", \"\");\n",
      "\n",
      "      ck = NotebookCellKind.Markup;\n",
      "      cellIndex = await insertCell(e, cellIndex, ck);\n",
      "    }\n",
      "\n",
      "    if (ck === undefined) {\n",
      "      cellIndex = await insertCell(e, cellIndex, NotebookCellKind.Markup);\n",
      "      ck = NotebookCellKind.Markup;\n",
      "    }\n",
      "\n",
      "    await appendTextToCell(e, cellIndex, textToken);\n",
      "\n",
      "    progress.report({ increment: 0.5, message: msgs.receivingTokens });\n",
      "  }\n",
      "\n",
      "  return FinishReason.length;\n",
      "}\n",
      "\n",
      "type ExtendedCreateChatCompletionRequest = CreateChatCompletionRequest & {\n",
      "  [key: string]: any;\n",
      "};\n",
      "\n",
      "function addParametersFromMetadata(nbMetadata: any, reqParams: CreateChatCompletionRequest) {\n",
      "  const metadataToReqParamMap = {\n",
      "    top_p: \"top_p\",\n",
      "    n: \"n\",\n",
      "    max_tokens: \"max_tokens\",\n",
      "    presence_penalty: \"presence_penalty\",\n",
      "    frequency_penalty: \"frequency_penalty\",\n",
      "    logit_bias: \"logit_bias\",\n",
      "    user: \"user\",\n",
      "  };\n",
      "\n",
      "  const extendedReqParams: ExtendedCreateChatCompletionRequest = reqParams;\n",
      "\n",
      "  for (const [metadataKey, reqParamKey] of Object.entries(metadataToReqParamMap)) {\n",
      "    if (nbMetadata && nbMetadata[metadataKey]) {\n",
      "      extendedReqParams[reqParamKey] = nbMetadata[metadataKey];\n",
      "    }\n",
      "  }\n",
      "\n",
      "  return reqParams;\n",
      "}\n",
      "\n",
      "async function getOpenAIApiKey(): Promise<string> {\n",
      "  let apiKey = workspace.getConfiguration().get<string>(configKeys.openAiKey);\n",
      "  if (!apiKey) {\n",
      "    apiKey = await window.showInputBox({\n",
      "      prompt: msgs.enterApiKey,\n",
      "      validateInput: (value) => (value.trim().length > 0 ? null : msgs.apiKeyCannotBeEmpty),\n",
      "    });\n",
      "\n",
      "    if (apiKey) {\n",
      "      await workspace.getConfiguration().update(configKeys.openAiKey, apiKey, ConfigurationTarget.Global);\n",
      "\n",
      "      await window.showInformationMessage(msgs.modelNotAccessible, { modal: true });\n",
      "    } else {\n",
      "      window.showErrorMessage(msgs.apiKeyRequired, { modal: true });\n",
      "      return \"\";\n",
      "    }\n",
      "  }\n",
      "\n",
      "  return apiKey;\n",
      "}\n",
      "\n",
      "function getTokenLimit(model: string): number | null {\n",
      "  let limit: number | null = null;\n",
      "\n",
      "  switch (model) {\n",
      "    case \"gpt-4\":\n",
      "    case \"gpt-4-0314\":\n",
      "      limit = 8192;\n",
      "      break;\n",
      "\n",
      "    case \"gpt-4-32k\":\n",
      "    case \"gpt-4-32k-0314\":\n",
      "      limit = 32768;\n",
      "      break;\n",
      "\n",
      "    case \"gpt-3.5-turbo\":\n",
      "    case \"gpt-3.5-turbo-0301\":\n",
      "      limit = 4096;\n",
      "      break;\n",
      "\n",
      "    default:\n",
      "      break;\n",
      "  }\n",
      "\n",
      "  return limit;\n",
      "}\n",
      "\n",
      "async function applyTokenReductionStrategies(\n",
      "  messages: ChatCompletionRequestMessage[],\n",
      "  tokenOverflowCount: number,\n",
      "  totalTokenCount: number,\n",
      "  limit: number,\n",
      "  model: string\n",
      "): Promise<ChatCompletionRequestMessage[] | null> {\n",
      "  const replacements = [\n",
      "    [uiText.removeOutput, /^Output from previous code:.*\\n?/gm, () => \"\"],\n",
      "    [uiText.removeProblems, /^Problems reported by VSCode from previous code:.*\\n?/gm, () => \"\"],\n",
      "    [uiText.removeSpaces, / /g, () => \"\"],\n",
      "    [uiText.removeLineBreaks, /\\n/g, () => \"\"],\n",
      "    [uiText.removePunctuation, /[.,;:!?]/g, () => \"\"],\n",
      "  ];\n",
      "\n",
      "  let strategies: TokenReductionStrategy[] = replacements.map(([label, pattern, replacementFn]) => ({\n",
      "    label: label as string,\n",
      "    apply: async () => {\n",
      "      return messages.map((message) => ({\n",
      "        ...message,\n",
      "        content: message.content.replace(pattern as RegExp, replacementFn as () => string),\n",
      "      }));\n",
      "    },\n",
      "  }));\n",
      "\n",
      "  for (const strategy of strategies) {\n",
      "    const reducedMessages = await strategy.apply();\n",
      "    const reducedTokenCount = countTotalTokens(reducedMessages, model);\n",
      "    const savedTokens = totalTokenCount - reducedTokenCount;\n",
      "    strategy.savedTokens = savedTokens;\n",
      "    strategy.description = `${savedTokens} tokens`;\n",
      "  }\n",
      "\n",
      "  strategies = strategies.filter((s) => (s.savedTokens ? s.savedTokens > 1 : false));\n",
      "\n",
      "  const maxPossibleSaving = strategies.map((x) => x.savedTokens ?? 0).reduce((prev, current) => prev + current);\n",
      "\n",
      "  if (maxPossibleSaving < tokenOverflowCount) {\n",
      "    window.showInformationMessage(\n",
      "      `If we applied every token reduction strategy available, you would still be ${\n",
      "        tokenOverflowCount - maxPossibleSaving\n",
      "      } over the limit of the '${model}' model. Please reduce the size of the content.`,\n",
      "      { modal: true }\n",
      "    );\n",
      "  }\n",
      "\n",
      "  const selectedStrategies = await window.showQuickPick(strategies, {\n",
      "    canPickMany: true,\n",
      "    title: uiText.tooManyTokens,\n",
      "    placeHolder: uiText.tooManyTokensPlaceholder,\n",
      "  });\n",
      "\n",
      "  if (!selectedStrategies) {\n",
      "    return null;\n",
      "  }\n",
      "\n",
      "  let reducedMessages = messages;\n",
      "  for (const strategy of selectedStrategies) {\n",
      "    reducedMessages = await strategy.apply(reducedMessages);\n",
      "  }\n",
      "\n",
      "  const reducedTokenCount = countTotalTokens(reducedMessages, model);\n",
      "  if (reducedTokenCount > limit) {\n",
      "    window.showErrorMessage(msgs.notEnoughSavings);\n",
      "    return null;\n",
      "  }\n",
      "\n",
      "  return reducedMessages;\n",
      "}\n",
      "\n",
      "function countTokens(text: string, model: any): number {\n",
      "  const enc = encoding_for_model(model);\n",
      "  const tokenCount = enc.encode(text).length;\n",
      "  enc.free();\n",
      "  return tokenCount;\n",
      "}\n",
      "\n",
      "function countTotalTokens(msgs: ChatCompletionRequestMessage[], model: string): number {\n",
      "  return msgs.reduce((accumulator, message) => {\n",
      "    return accumulator + countTokens(message.content, model);\n",
      "  }, 0);\n",
      "}\n",
      " \n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import List\n",
    "import re\n",
    "\n",
    "def print_file_content(file_path: Path):\n",
    "    content = file_path.read_text()\n",
    "\n",
    "    # Remove single-line comments\n",
    "    content = re.sub(r\"//.*\", \"\", content)\n",
    "\n",
    "    # Remove multi-line comments\n",
    "    content = re.sub(r\"/\\*[\\s\\S]*?\\*/\", \"\", content)\n",
    "\n",
    "    print(f\"\\n\\nFile: '{file_path}'\")\n",
    "    print(\"\\n\", content, \"\")\n",
    "\n",
    "def process_files(file_patterns: List[str]):\n",
    "    for pattern in file_patterns:\n",
    "        for file in Path().glob(pattern):\n",
    "            print_file_content(file)\n",
    "\n",
    "file_patterns = [\"../../src/completion.ts\"]\n",
    "process_files(file_patterns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I'd like to separate completion.ts into multiple files. Advise me on a good way to separate the logic in completion.ts in a way that would be intuitive to software engineers reading the code.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "assistant"
    ]
   },
   "source": [
    "A good way to separate the logic in `completion.ts` would be to create separate files for different concerns. Here's a suggested structure:\n",
    "\n",
    "1. `config.ts`: Contains functions related to configuration and API key management.\n",
    "   - `getOpenAIApiKey()`\n",
    "   - `getTokenLimit(model: string)`\n",
    "\n",
    "2. `tokenUtils.ts`: Contains functions related to token counting and reduction strategies.\n",
    "   - `countTokens(text: string, model: any)`\n",
    "   - `countTotalTokens(msgs: ChatCompletionRequestMessage[], model: string)`\n",
    "   - `applyTokenReductionStrategies(...)`\n",
    "\n",
    "3. `requestUtils.ts`: Contains functions related to creating and managing API requests.\n",
    "   - `addParametersFromMetadata(nbMetadata: any, reqParams: CreateChatCompletionRequest)`\n",
    "   - `generateCompletion(...)`\n",
    "\n",
    "4. `cellUtils.ts`: Contains functions related to notebook cell manipulation.\n",
    "   - `insertCell(e, cellIndex, ck, language?)`\n",
    "   - `appendTextToCell(e, cellIndex, textToken)`\n",
    "   - `convertCellsToMessages(cellIndex, completionType)`\n",
    "\n",
    "5. `completion.ts`: Contains the main logic for handling completions and imports the necessary functions from the other files.\n",
    "\n",
    "After separating the logic into these files, make sure to import the necessary functions in `completion.ts` and other files where they are used. This structure should make it easier for software engineers to understand and navigate the code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "model": "gpt-4",
 "nbformat": 4,
 "nbformat_minor": 2
}
