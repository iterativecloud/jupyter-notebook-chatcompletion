{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "system"
    ]
   },
   "source": [
    "> You are a code generator and reviewer that can only answer with python code."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File: 'example.ts'\n",
    "\n",
    "```typescript\n",
    "function greet(name: string) {\n",
    "  console.log(\"Hello, \" + name);\n",
    "}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add the age of the user to the console log.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": [
     "assistant"
    ]
   },
   "outputs": [],
   "source": [
    "with open('example.ts', 'w') as f: f.write(\"\"\"\n",
    "function greet(name: string, age : number) {\n",
    "  console.log(`Hello, ${name}. You are ${age} years old.`);\n",
    "}\n",
    "\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File: '../../src/finishReason.ts'\n",
    "\n",
    "```typescript\n",
    "export enum FinishReason {\n",
    "  length,\n",
    "  contentFilter,\n",
    "  stop,\n",
    "  null,\n",
    "  cancelled\n",
    "}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**My compiler says that FinishReason \"timeout\" is missing. Fix the issue.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "assistant"
    ]
   },
   "outputs": [],
   "source": [
    "with open('../../src/finishReason.ts', 'w') as f: f.write(\"\"\"\n",
    "export enum FinishReason {\n",
    "  length,\n",
    "  contentFilter,\n",
    "  stop,\n",
    "  null,\n",
    "  cancelled,\n",
    "  timeout\n",
    "}\n",
    "\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "File: '../../README.md'\n",
      "\n",
      " # ðŸ’¬ Jupyter Notebook ChatCompletion for VSCode\n",
      "\n",
      "Jupyter Notebook ChatCompletion is VSCode extension that brings the power of OpenAI's ChatCompletion API to your Jupyter Notebooks!\n",
      "\n",
      "With this extension, you can generate completions for your code cells, making it easier than ever to experiment with different models and parameters. The best part? The completions are streamed, so you can cancel them at any time! \n",
      "\n",
      "Plus, you can optionally send cell outputs and VSCode problems detected on the cell as part of the prompt to the ChatCompletion API, giving you even more ways to add context to your notebook cell generation.\n",
      "\n",
      "![](https:\n",
      "\n",
      "## ðŸŒŸ Features \n",
      "\n",
      "- Streamed completions: Generate completions and see the first results on-the-fly.\n",
      "- Cancel early: Don't like what you see? You can cancel an ongoing completion at any time.\n",
      "- Send Jupyter notebook cell outputs and VSCode problems to the ChatCompletion API for better context-aware completions.\n",
      "- Store API parameters within the notebook instead of the workspace, making side-by-side experimentations easier and more reproducible.\n",
      "- Customize your completions with various API parameters, including temperature, top_p, max tokens, and more.\n",
      "- Manage Chat roles: set a notebook cell to the  System or Assistant role, giving you the same level of control as in the OpenAI Playground.\n",
      "\n",
      "## ðŸš€ Getting Started \n",
      "\n",
      "1. Install the Jupyter Notebook ChatCompletion extension from the VSCode marketplace.\n",
      "2. Open a Jupyter Notebook in VSCode.\n",
      "3. Start generating completions for your code cells using the extension commands!\n",
      "\n",
      "## ðŸŽ® Commands \n",
      "\n",
      "- **Complete with this Cell and Above** (`ctrl+shift+enter`): Generate completions using the current cell and all cells above it.\n",
      "- **Complete with this Cell** (`ctrl+shift+pagedown`): Generate completions using only the current cell.\n",
      "- **Set Temperature**: Adjust the temperature parameter for controlling the randomness of the completions.\n",
      "- **Set Model**: Choose the OpenAI model to use for generating completions.\n",
      "- **Set Chat Role to Assistant**: Set the role of the current cell to \"assistant\".\n",
      "- **Set Chat Role to System**: Set the role of the current cell to \"system\".\n",
      "- **Set Top P Parameter**: Set the `top_p` parameter for nucleus sampling, where the model considers the results of the tokens with top_p probability mass (e.g., 0.1 means only the tokens comprising the top 10% probability mass are considered).\n",
      "- **Set Max Tokens Parameter**: Set the `max_tokens` parameter to limit the maximum number of tokens generated in the chat completion.\n",
      "- **Set Presence Penalty Parameter**: Set the `presence_penalty` parameter to penalize new tokens based on whether they appear in the text so far, influencing the model's likelihood to talk about new topics.\n",
      "- **Set Frequency Penalty Parameter**: Set the `frequency_penalty` parameter to penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.\n",
      "- **Set Logit Bias Parameter**: Set the `logit_bias` parameter to modify the likelihood of specified tokens appearing in the completion by providing a JSON object that maps tokens to their associated bias values.\n",
      "- **Set User Parameter**: Set the `user` parameter to provide a unique identifier representing your end-user, which can help OpenAI monitor and detect abuse.\n",
      "\n",
      "## âš™ï¸ Configuration \n",
      "\n",
      "To use the extension, you'll need to provide your OpenAI API Key. You can do this by setting the `notebook-chatcompletion.openaiApiKey` configuration property in your VSCode settings. You will also be prompted for the API key on first use if it hasn't been defined yet.\n",
      "\n",
      "## ðŸ„ Full freedom\n",
      "In stark constrast to existing GPT-related VSCode extensions, extra care was taken to not steer the completion in any way that might conflict with the intent of your notebook. You can therefore expect the same level of flexibility as with the OpenAI Playground, but combined with the powerful features of Jupyter notebooks.\n",
      "\n",
      "This extension will only include the following system message by default: \n",
      "\"```Format your answer as markdown. If you include a markdown code block, specify the language.```\"\n",
      "This default system message increases the chance that the extension will detect python code as code cell instead of generic code-block (unknown language). You can set a notebook to the role \"System\" to define your own system message.\n",
      "\n",
      "If you want an experience closer to ChatGPT, you can try setting the system message to: \"```You are ChatGPT, a large language model trained by OpenAI. Answer as concisely as possible. Knowledge cutoff: {knowledge_cutoff} Current date: {current_date}```\"\n",
      "\n",
      "## ðŸ›  Support\n",
      "\n",
      "If you encounter any issues or have questions, please head over to [GitHub Issues](https:\n",
      "\n",
      "## ðŸŒˆ Upcoming\n",
      "\n",
      "I am working to improve the Jupyter Notebook ChatCompletion extension. Some of the next exciting features I am working towards are **Automatic truncation when reaching max token limit**, **Polyglot Notebook support** and supporting locally run **HuggingFace LLM models**. Stay tuned for more updates and enhancements! \n",
      "\n",
      "\n",
      "File: '../../src/extension.ts'\n",
      "\n",
      " import axios from \"axios\";\n",
      "import { ExtensionContext, NotebookEdit, NotebookRange, ProgressLocation, WorkspaceEdit, commands, window, workspace } from \"vscode\";\n",
      "import { generateCompletion } from \"./completion\";\n",
      "import { CompletionType } from \"./completionType\";\n",
      "import { FinishReason } from \"./finishReason\";\n",
      "import { ChatCompletionRequestMessageRoleEnum as Roles } from \"openai\";\n",
      "import { errorMessages, models, msgs, prompts } from \"./constants\";\n",
      "\n",
      "export async function activate(ctx: ExtensionContext) {\n",
      "  const regCmd = (cmd: string, handler: (...args: any[]) => any) =>\n",
      "    ctx.subscriptions.push(commands.registerCommand(\"notebook-chatcompletion.\" + cmd, handler));\n",
      "\n",
      "  regCmd(\"sendCellAndAbove\", (...args) => genCells(args, CompletionType.currentCellAndAbove));\n",
      "  regCmd(\"sendCell\", (...args) => genCells(args, CompletionType.currentCell));\n",
      "  regCmd(\"setRoleAssistant\", () => setRole(Roles.Assistant));\n",
      "  regCmd(\"setRoleSystem\", () => setRole(Roles.System));\n",
      "  regCmd(\"setModel\", setModel);\n",
      "  regCmd(\"setTemperature\", () => setParam(prompts.temperature, \"temperature\", parseFloat, (v) => parseFloat(v) >= 0 && parseFloat(v) <= 1));\n",
      "  regCmd(\"setTopP\", () => setParam(prompts.topP, \"top_p\", parseFloat, (v) => parseFloat(v) >= 0 && parseFloat(v) <= 1));\n",
      "  regCmd(\"setMaxTokens\", () => setParam(prompts.maxTokens, \"max_tokens\", parseInt, (v) => parseInt(v) > 0));\n",
      "  regCmd(\"setPresencePenalty\", () =>\n",
      "    setParam(prompts.presencePenalty, \"presence_penalty\", parseFloat, (v) => parseFloat(v) >= 0 && parseFloat(v) <= 1)\n",
      "  );\n",
      "  regCmd(\"setFrequencyPenalty\", () =>\n",
      "    setParam(prompts.frequencyPenalty, \"frequency_penalty\", parseFloat, (v) => parseFloat(v) >= 0 && parseFloat(v) <= 1)\n",
      "  );\n",
      "  regCmd(\"setLogitBias\", () =>\n",
      "    setParam(prompts.logitBias, \"logit_bias\", JSON.parse, (v) => {\n",
      "      try {\n",
      "        JSON.parse(v);\n",
      "        return null;\n",
      "      } catch (e) {\n",
      "        return msgs.logitValidJson;\n",
      "      }\n",
      "    })\n",
      "  );\n",
      "  regCmd(\"setUser\", () =>\n",
      "    setParam(\n",
      "      prompts.user,\n",
      "      \"user\",\n",
      "      (v) => v,\n",
      "      (v) => v.trim().length > 0\n",
      "    )\n",
      "  );\n",
      "}\n",
      "\n",
      "async function genCells(args: any, completionType: CompletionType) {\n",
      "  let cellIndex = args[0]?.index;\n",
      "  if (!cellIndex) {\n",
      "    cellIndex = window.activeNotebookEditor!.selection.end - 1;\n",
      "  }\n",
      "  window.activeNotebookEditor!.selection = new NotebookRange(cellIndex, cellIndex);\n",
      "\n",
      "  window.withProgress(\n",
      "    {\n",
      "      title: msgs.genNextCell,\n",
      "      location: ProgressLocation.Notification,\n",
      "      cancellable: true,\n",
      "    },\n",
      "    async (progress, cancelToken) => {\n",
      "      try {\n",
      "        let finishReason = FinishReason.null;\n",
      "        finishReason = await generateCompletion(cellIndex, completionType, progress, cancelToken);\n",
      "        await commands.executeCommand(\"notebook.cell.quitEdit\");\n",
      "\n",
      "        switch (finishReason) {\n",
      "          case FinishReason.length:\n",
      "          case FinishReason.stop:\n",
      "            window.showInformationMessage(msgs.compCompleted);\n",
      "            progress.report({ increment: 100 });\n",
      "            break;\n",
      "          case FinishReason.cancelled:\n",
      "            window.showInformationMessage(msgs.compCancelled);\n",
      "            progress.report({ increment: 100 });\n",
      "            break;\n",
      "          case FinishReason.contentFilter:\n",
      "            window.showErrorMessage(msgs.apiViolation);\n",
      "            progress.report({ increment: 100 });\n",
      "            break;\n",
      "          default:\n",
      "            throw new Error(errorMessages.unhandledFinishReason);\n",
      "        }\n",
      "      } catch (e: any) {\n",
      "        if (e.code && e.code === \"ECONNRESET\") {\n",
      "          window.showErrorMessage(`${msgs.compFailed}: ${e.message}`, {\n",
      "            detail: msgs.connectionReset,\n",
      "            modal: true,\n",
      "          });\n",
      "          return;\n",
      "        }\n",
      "\n",
      "        if (e instanceof axios.Cancel) {\n",
      "          window.showInformationMessage(`${msgs.compCancelled}: ${e.message}`);\n",
      "          return;\n",
      "        }\n",
      "        let detail = \"\";\n",
      "        if (e.response) {\n",
      "          switch (e.response.status) {\n",
      "            case 400:\n",
      "              detail = errorMessages.maxTokenLimit;\n",
      "              break;\n",
      "            case 401:\n",
      "              detail = errorMessages.apiKeyOrg;\n",
      "              break;\n",
      "            case 404:\n",
      "              detail = errorMessages.endpointModel;\n",
      "              break;\n",
      "            case 429:\n",
      "              detail = errorMessages.rateLimit;\n",
      "              break;\n",
      "            case 500:\n",
      "              detail = errorMessages.serverError;\n",
      "              break;\n",
      "          }\n",
      "        }\n",
      "        detail += e instanceof Error ? e.message : String(e);\n",
      "        window.showErrorMessage(`${msgs.compFailed}: ${e.message}`, {\n",
      "          detail,\n",
      "          modal: true,\n",
      "        });\n",
      "      }\n",
      "    }\n",
      "  );\n",
      "}\n",
      "\n",
      "async function setModel() {\n",
      "  const model = await window.showQuickPick(models, {\n",
      "    placeHolder: prompts.selectModel,\n",
      "  });\n",
      "\n",
      "  if (model) {\n",
      "    const editor = window.activeNotebookEditor!;\n",
      "    const edit = new WorkspaceEdit();\n",
      "    edit.set(editor.notebook.uri, [\n",
      "      NotebookEdit.updateNotebookMetadata({\n",
      "        custom: {\n",
      "          ...editor.notebook.metadata.custom,\n",
      "          model: model,\n",
      "        },\n",
      "      }),\n",
      "    ]);\n",
      "    await workspace.applyEdit(edit);\n",
      "  }\n",
      "}\n",
      "\n",
      "async function setParam(prompt: string, key: string, parseFn: (v: string) => any, validateFn: (v: string) => any) {\n",
      "  const editor = window.activeNotebookEditor!;\n",
      "  const value = await window.showInputBox({\n",
      "    prompt,\n",
      "    validateInput: validateFn,\n",
      "  });\n",
      "\n",
      "  if (value) {\n",
      "    const edit = new WorkspaceEdit();\n",
      "    edit.set(editor.notebook.uri, [\n",
      "      NotebookEdit.updateNotebookMetadata({\n",
      "        custom: {\n",
      "          ...editor.notebook.metadata.custom,\n",
      "          [key]: parseFn(value),\n",
      "        },\n",
      "      }),\n",
      "    ]);\n",
      "    await workspace.applyEdit(edit);\n",
      "  }\n",
      "}\n",
      "\n",
      "async function setRole(role: Roles) {\n",
      "  const editor = window.activeNotebookEditor!;\n",
      "  const cell = editor.notebook.cellAt(editor.selection.end - 1);\n",
      "  const edit = new WorkspaceEdit();\n",
      "  edit.set(cell.notebook.uri, [\n",
      "    NotebookEdit.updateCellMetadata(cell.index, {\n",
      "      custom: { metadata: { tags: [role] } },\n",
      "    }),\n",
      "  ]);\n",
      "  await workspace.applyEdit(edit);\n",
      "}\n",
      " \n",
      "\n",
      "\n",
      "File: '../../src/completion.ts'\n",
      "\n",
      " import axios, { AxiosResponse } from \"axios\";\n",
      "import { Configuration, CreateChatCompletionRequest, CreateChatCompletionResponse, OpenAIApi } from \"openai\";\n",
      "import { CancellationToken, NotebookCellKind, NotebookEdit, NotebookRange, WorkspaceEdit, window, workspace } from \"vscode\";\n",
      "import { appendTextToCell, convertCellsToMessages, insertCell } from \"./cellUtils\";\n",
      "import { CompletionType } from \"./completionType\";\n",
      "import { addParametersFromMetadata as addNotebookConfigParams, getOpenAIApiKey, getTokenLimit } from \"./config\";\n",
      "import { msgs } from \"./constants\";\n",
      "import { FinishReason } from \"./finishReason\";\n",
      "import { bufferWholeChunks, streamChatCompletion } from \"./streamUtils\";\n",
      "import { applyTokenReductions, countTokens } from \"./tokenUtils\";\n",
      "import { UIProgress } from \"./uiProgress\";\n",
      "\n",
      "const output = window.createOutputChannel(\"Notebook ChatCompletion\");\n",
      "\n",
      "async function streamResponse(\n",
      "  response: AxiosResponse<CreateChatCompletionResponse, any>,\n",
      "  cancelToken: CancellationToken,\n",
      "  cellIndex: number,\n",
      "  ck: NotebookCellKind | undefined,\n",
      "  progress: UIProgress\n",
      ") {\n",
      "  const editor = window.activeNotebookEditor!;\n",
      "\n",
      "  for await (let textToken of bufferWholeChunks(streamChatCompletion(response, cancelToken))) {\n",
      "    if (Object.values(FinishReason).includes(textToken as FinishReason)) {\n",
      "      const currentCell = window.activeNotebookEditor!.notebook.cellAt(cellIndex);\n",
      "      const text = currentCell.document.getText();\n",
      "\n",
      "      if (!/\\S/.test(text)) {\n",
      "        const edit = new WorkspaceEdit();\n",
      "        edit.set(currentCell.notebook.uri, [NotebookEdit.deleteCells(new NotebookRange(currentCell.index, currentCell.index + 1))]);\n",
      "        await workspace.applyEdit(edit);\n",
      "      }\n",
      "\n",
      "      return textToken as FinishReason;\n",
      "    } else {\n",
      "      output.append(textToken.toString());\n",
      "    }\n",
      "\n",
      "    if (typeof textToken !== \"string\") {\n",
      "      throw new Error(`Unknown stream result: ${textToken}`);\n",
      "    }\n",
      "\n",
      "    if (textToken.includes(\"```python\\n\")) {\n",
      "      ck = NotebookCellKind.Code;\n",
      "\n",
      "      cellIndex = await insertCell(editor, cellIndex, ck, \"python\");\n",
      "      textToken = textToken.replace(\"```python\\n\", \"\");\n",
      "    } else if (textToken.includes(\"```\") && ck === NotebookCellKind.Code) {\n",
      "      textToken = textToken.replace(\"```\", \"\");\n",
      "\n",
      "      ck = NotebookCellKind.Markup;\n",
      "      cellIndex = await insertCell(editor, cellIndex, ck);\n",
      "    }\n",
      "\n",
      "    if (ck === undefined) {\n",
      "      cellIndex = await insertCell(editor, cellIndex, NotebookCellKind.Markup);\n",
      "      ck = NotebookCellKind.Markup;\n",
      "    }\n",
      "\n",
      "    await appendTextToCell(editor, cellIndex, textToken);\n",
      "\n",
      "    progress.report({ increment: 0.5, message: msgs.receivingTokens });\n",
      "  }\n",
      "\n",
      "  return FinishReason.length;\n",
      "}\n",
      "\n",
      "export async function generateCompletion(\n",
      "  cellIndex: number,\n",
      "  completionType: CompletionType,\n",
      "  progress: UIProgress,\n",
      "  cancelToken: CancellationToken\n",
      "): Promise<FinishReason> {\n",
      "  const e = window.activeNotebookEditor!;\n",
      "  let messages = await convertCellsToMessages(cellIndex, completionType);\n",
      "  let ck: NotebookCellKind | undefined = undefined;\n",
      "\n",
      "  const openaiApiKey = await getOpenAIApiKey();\n",
      "\n",
      "  if (!openaiApiKey) {\n",
      "    throw new Error(msgs.apiKeyNotSet);\n",
      "  }\n",
      "\n",
      "  const openai = new OpenAIApi(new Configuration({ apiKey: openaiApiKey }));\n",
      "\n",
      "  const tokenSource = axios.CancelToken.source();\n",
      "  cancelToken.onCancellationRequested(tokenSource.cancel);\n",
      "\n",
      "  const nbMetadata = e.notebook.metadata.custom;\n",
      "\n",
      "  const defaultModel = workspace.getConfiguration().get<string>(\"notebook-chatcompletion.defaultModel\");\n",
      "\n",
      "  const model = nbMetadata?.model ?? defaultModel;\n",
      "  const temperature = nbMetadata?.temperature ?? 0;\n",
      "\n",
      "  const limit = getTokenLimit(model);\n",
      "\n",
      "  const msgText = JSON.stringify(messages);\n",
      "  const totalTokenCount = countTokens(msgText, model);\n",
      "\n",
      "  if (limit !== null && totalTokenCount > limit) {\n",
      "    const tokenOverflow = limit - totalTokenCount;\n",
      "\n",
      "    const msgText = messages.map((x) => x.content).join();\n",
      "    const contentTokenCount = countTokens(msgText, model);\n",
      "\n",
      "    const reducedMessages = await applyTokenReductions(messages, tokenOverflow, contentTokenCount, limit, model);\n",
      "\n",
      "    if (!reducedMessages) {\n",
      "      return FinishReason.cancelled;\n",
      "    }\n",
      "\n",
      "    messages = reducedMessages;\n",
      "  }\n",
      "\n",
      "  let reqParams: CreateChatCompletionRequest = {\n",
      "    model: model,\n",
      "    messages: messages,\n",
      "    stream: true,\n",
      "    temperature: temperature,\n",
      "  };\n",
      "\n",
      "  if (limit) {\n",
      "    const reducedMsgText = JSON.stringify(messages);\n",
      "    const reducedTokenCount = countTokens(reducedMsgText, model);\n",
      "    reqParams.max_tokens = limit - reducedTokenCount;\n",
      "\n",
      "    if (reqParams.max_tokens < 1) {\n",
      "      const result = await window.showInformationMessage(\n",
      "        `The request is estimated to be ${-reqParams.max_tokens} tokens over the limit (including the input) and will likely be rejected from the OpenAI API. Do you still want to proceed?`,\n",
      "        { modal: true },\n",
      "        \"Yes\"\n",
      "      );\n",
      "      if (result !== \"Yes\") {\n",
      "        return FinishReason.cancelled;\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "\n",
      "  reqParams = addNotebookConfigParams(nbMetadata, reqParams);\n",
      "\n",
      "  output.appendLine(\"\\n\" + JSON.stringify(reqParams, undefined, 2) + \"\\n\");\n",
      "  progress.report({ increment: 1, message: msgs.sendingRequest });\n",
      "\n",
      "  const response = await openai.createChatCompletion(reqParams, {\n",
      "    cancelToken: tokenSource.token,\n",
      "    responseType: \"stream\",\n",
      "  });\n",
      "\n",
      "  return await streamResponse(response, cancelToken, cellIndex, ck, progress);\n",
      "}\n",
      " \n",
      "\n",
      "\n",
      "File: '../../src/cellUtils.ts'\n",
      "\n",
      " import {\n",
      "  ChatCompletionRequestMessage,\n",
      "  ChatCompletionRequestMessageRoleEnum,\n",
      "} from \"openai\";\n",
      "import {\n",
      "  NotebookCellKind,\n",
      "  NotebookEdit,\n",
      "  NotebookEditor,\n",
      "  NotebookRange,\n",
      "  Range,\n",
      "  WorkspaceEdit,\n",
      "  commands,\n",
      "  languages,\n",
      "  window,\n",
      "  workspace,\n",
      "} from \"vscode\";\n",
      "import { CompletionType } from \"./completionType\";\n",
      "\n",
      "const ADDITIONAL_PROMPT_INFO_MESSAGE =\n",
      "  \"Select any additional information you want to include in the prompt\";\n",
      "\n",
      "export async function appendTextToCell(\n",
      "  editor: NotebookEditor,\n",
      "  cellIndex: number,\n",
      "  textToken: string\n",
      ") {\n",
      "  const existingCell = editor.notebook.cellAt(cellIndex);\n",
      "\n",
      "  \n",
      "  if (existingCell.document.getText().length === 0 && !/\\S/.test(textToken)) {\n",
      "    return;\n",
      "  }\n",
      "\n",
      "  if (textToken.startsWith(\"\\n\")) {\n",
      "    \n",
      "    if (existingCell.document.positionAt(1).character === 0) {\n",
      "      \n",
      "      if (textToken.length > 1) {\n",
      "        textToken = textToken.substring(1);\n",
      "      } else {\n",
      "        return;\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "\n",
      "  const edit = new WorkspaceEdit();\n",
      "  edit.insert(\n",
      "    existingCell.document.uri,\n",
      "    existingCell.document.positionAt(9999999999),\n",
      "    textToken\n",
      "  );\n",
      "  await workspace.applyEdit(edit);\n",
      "}\n",
      "\n",
      "export async function insertCell(\n",
      "  editor: NotebookEditor,\n",
      "  cellIndex: number,\n",
      "  cellKind: NotebookCellKind,\n",
      "  languageId: string = \"markdown\"\n",
      ") {\n",
      "  \n",
      "  const existingCell = editor.notebook.cellAt(cellIndex);\n",
      "  if (existingCell.document.getText().endsWith(\"\\n\")) {\n",
      "    const edit = new WorkspaceEdit();\n",
      "    const lastLineRange = existingCell.document.validateRange(\n",
      "      new Range(\n",
      "        existingCell.document.lineCount - 2,\n",
      "        9999999999,\n",
      "        existingCell.document.lineCount,\n",
      "        0\n",
      "      )\n",
      "    );\n",
      "\n",
      "    edit.delete(existingCell.document.uri, lastLineRange);\n",
      "    await workspace.applyEdit(edit);\n",
      "  }\n",
      "\n",
      "  await commands.executeCommand(\"notebook.cell.quitEdit\");\n",
      "\n",
      "  if (cellKind === NotebookCellKind.Code && languageId === \"python\") {\n",
      "    await commands.executeCommand(\"notebook.cell.insertCodeCellBelow\", [\n",
      "      { index: cellIndex },\n",
      "    ]);\n",
      "  } else {\n",
      "    await commands.executeCommand(\"notebook.cell.insertMarkdownCellBelow\", [\n",
      "      { index: cellIndex },\n",
      "    ]);\n",
      "  }\n",
      "\n",
      "  cellIndex++;\n",
      "\n",
      "  const edit = new WorkspaceEdit();\n",
      "  let cell = editor.notebook.cellAt(cellIndex);\n",
      "  edit.set(cell.notebook.uri, [\n",
      "    NotebookEdit.updateCellMetadata(cell.index, {\n",
      "      custom: { metadata: { tags: [\"assistant\"] } },\n",
      "    }),\n",
      "  ]);\n",
      "  await workspace.applyEdit(edit);\n",
      "\n",
      "  return cellIndex;\n",
      "}\n",
      "\n",
      "export async function convertCellsToMessages(\n",
      "  cellIndex: number,\n",
      "  completionType: CompletionType\n",
      "): Promise<ChatCompletionRequestMessage[]> {\n",
      "  const editor = window.activeNotebookEditor!;\n",
      "  const notebook = editor.notebook;\n",
      "\n",
      "  const startCellIndex =\n",
      "    completionType === CompletionType.currentCellAndAbove ? 0 : cellIndex;\n",
      "\n",
      "  const diagnostics = languages\n",
      "    .getDiagnostics()\n",
      "    .filter(([uri]) => uri.path === notebook.uri.path)\n",
      "    .flatMap(([, diag]) => diag);\n",
      "\n",
      "  const cellInfos = notebook\n",
      "    .getCells(new NotebookRange(startCellIndex, cellIndex + 1))\n",
      "    .map((cell) => {\n",
      "      const problems = diagnostics.filter(\n",
      "        (d) => cell.document.validateRange(d.range) === d.range\n",
      "      );\n",
      "      const nonImgOutputs = cell.outputs\n",
      "        .flatMap((o) => o.items.filter((i) => !i.mime.startsWith(\"image\")))\n",
      "        .map((i) => i.data.toString());\n",
      "      return { cell, problems, nonImgOutputs };\n",
      "    });\n",
      "\n",
      "  var messages: ChatCompletionRequestMessage[] = [];\n",
      "\n",
      "  cellInfos.forEach(({ cell, problems, nonImgOutputs }) => {\n",
      "    let role: ChatCompletionRequestMessageRoleEnum =\n",
      "      ChatCompletionRequestMessageRoleEnum.User;\n",
      "    const tags: string[] = cell.metadata?.custom?.metadata?.tags;\n",
      "\n",
      "    if (tags && tags.length > 0) {\n",
      "      role = tags[0] as ChatCompletionRequestMessageRoleEnum;\n",
      "    }\n",
      "\n",
      "    let cellContent = cell.document.getText();\n",
      "\n",
      "    if (cell.kind === NotebookCellKind.Code) {\n",
      "      cellContent = `\\`\\`\\`python \\n${cellContent}\\n\\`\\`\\``;\n",
      "    }\n",
      "\n",
      "    messages.push({ role: role, content: cellContent });\n",
      "\n",
      "    if (problems.length > 0) {\n",
      "      messages.push({\n",
      "        role: role ?? \"user\",\n",
      "        content:\n",
      "          \"Problems reported by VSCode from previous code:\\n\" +\n",
      "          problems.map((p) => `${p.code}: ${p.message}`),\n",
      "      });\n",
      "    }\n",
      "\n",
      "    if (nonImgOutputs.length) {\n",
      "      nonImgOutputs.forEach((output) =>\n",
      "        messages.push({\n",
      "          role: role ?? \"user\",\n",
      "          content: \"Output from previous code:\\n\" + output,\n",
      "        })\n",
      "      );\n",
      "    }\n",
      "  });\n",
      "\n",
      "  const totalLengthUserMessages = messages.reduce(\n",
      "    (accumulator, currentValue) => {\n",
      "      return accumulator + currentValue.content.length;\n",
      "    },\n",
      "    0\n",
      "  );\n",
      "\n",
      "  const systemMessages = messages.filter(\n",
      "    (m) => m.role === ChatCompletionRequestMessageRoleEnum.System\n",
      "  );\n",
      "\n",
      "  \n",
      "  \n",
      "  if (systemMessages.length === 0 && totalLengthUserMessages > 32) {\n",
      "    messages.push({\n",
      "      role: ChatCompletionRequestMessageRoleEnum.System,\n",
      "      content:\n",
      "        \"Format your answer as markdown. If you include a markdown code block, specify the language.\",\n",
      "    });\n",
      "  }\n",
      "\n",
      "  return messages;\n",
      "}\n",
      " \n",
      "\n",
      "\n",
      "File: '../../src/streamUtils.ts'\n",
      "\n",
      " import axios, { AxiosResponse } from \"axios\";\n",
      "import { CreateChatCompletionResponse } from \"openai\";\n",
      "import { CancellationToken, window } from \"vscode\";\n",
      "import { FinishReason } from \"./finishReason\";\n",
      "\n",
      "export async function* streamChatCompletion(\n",
      "  response: AxiosResponse<CreateChatCompletionResponse, AsyncIterable<Buffer>>,\n",
      "  token: CancellationToken\n",
      "): AsyncGenerator<string | FinishReason, void, undefined> {\n",
      "  \n",
      "  const dataStream = response.data as unknown as AsyncIterable<Buffer>;\n",
      "\n",
      "  for await (const chunk of dataStream) {\n",
      "    if (token.isCancellationRequested) {\n",
      "      throw new axios.Cancel(\"ChatCompletion API request cancelled by user\");\n",
      "    }\n",
      "\n",
      "    const lines = chunk\n",
      "      .toString(\"utf8\")\n",
      "      .split(\"\\n\")\n",
      "      .filter((line) => line.trim().startsWith(\"data: \"));\n",
      "\n",
      "    for (const line of lines) {\n",
      "      const message = line.replace(/^data: /, \"\");\n",
      "      const json = JSON.parse(message);\n",
      "\n",
      "      const content = json.choices[0].delta.content;\n",
      "\n",
      "      if (content !== undefined && content !== \"\") {\n",
      "        yield content;\n",
      "      }\n",
      "\n",
      "      const finishReason = json.choices[0].finish_reason;\n",
      "\n",
      "      switch (finishReason) {\n",
      "        case \"length\": \n",
      "          yield FinishReason.length;\n",
      "\n",
      "        case \"content_filter\": \n",
      "        yield FinishReason.contentFilter;\n",
      "\n",
      "        case \"stop\": \n",
      "          yield FinishReason.stop;\n",
      "          return;\n",
      "        case null:\n",
      "        case undefined:\n",
      "        case \"null\": \n",
      "          continue;\n",
      "        default: \n",
      "          throw new Error(\"Unhandled stop_reason:\" + finishReason);\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "export async function* bufferWholeChunks(\n",
      "  stream: AsyncGenerator<string | FinishReason, void, undefined>\n",
      "): AsyncGenerator<string | FinishReason, void, undefined> {\n",
      "  let buffer = \"\";\n",
      "  let value: string | void | FinishReason = undefined;\n",
      "\n",
      "  while ((value = (await stream.next()).value)) {\n",
      "    if (\n",
      "      typeof value === \"string\" &&\n",
      "      !value.includes(\"\\n\") &&\n",
      "      !value.includes(\" \") &&\n",
      "      !value.includes(\"-\") &&\n",
      "      !value.includes(\"<\") &&\n",
      "      !value.includes(\">\") &&\n",
      "      !value.includes(\"(\") &&\n",
      "      !value.includes(\")\") &&\n",
      "      !value.includes(\",\") &&\n",
      "      !value.includes(\".\") &&\n",
      "      !value.includes(\"'\") &&\n",
      "      !value.includes('\"')\n",
      "    ) {\n",
      "      buffer += value;\n",
      "    } else {\n",
      "      if (buffer.length > 0) {\n",
      "        if (typeof value === \"string\") {\n",
      "          yield buffer + value;\n",
      "        } else {\n",
      "          if (buffer !== \"\") {\n",
      "            yield buffer;\n",
      "          }\n",
      "          yield value;\n",
      "        }\n",
      "        buffer = \"\";\n",
      "      } else {\n",
      "        yield value;\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      " \n",
      "\n",
      "\n",
      "File: '../../src/tokenUtils.ts'\n",
      "\n",
      " import { encoding_for_model } from \"@dqbd/tiktoken\";\n",
      "import { ChatCompletionRequestMessage } from \"openai\";\n",
      "import { QuickPickItem, window } from \"vscode\";\n",
      "import { msgs, uiText } from \"./constants\";\n",
      "\n",
      "export async function applyTokenReductions(\n",
      "  messages: ChatCompletionRequestMessage[],\n",
      "  tokenOverflowCount: number,\n",
      "  totalTokenCount: number,\n",
      "  limit: number,\n",
      "  model: string\n",
      "): Promise<ChatCompletionRequestMessage[] | null> {\n",
      "  const replacements = [\n",
      "    [uiText.removeOutput, /^Output from previous code:.*\\n?/gm, () => \"\"],\n",
      "    [uiText.removeProblems, /^Problems reported by VSCode from previous code:.*\\n?/gm, () => \"\"],\n",
      "    [uiText.removeSpaces, / /g, () => \"\"],\n",
      "    [uiText.removeLineBreaks, /\\n/g, () => \"\"],\n",
      "    [uiText.removePunctuation, /[.,;:!?]/g, () => \"\"],\n",
      "  ];\n",
      "\n",
      "  type TokenReductionStrategy = QuickPickItem & {\n",
      "    apply: Function;\n",
      "    savedTokens?: number;\n",
      "  };\n",
      "\n",
      "  let strategies: TokenReductionStrategy[] = replacements.map(([label, pattern, replacementFn]) => ({\n",
      "    label: label as string,\n",
      "    apply: async () => {\n",
      "      return messages.map((message) => ({\n",
      "        ...message,\n",
      "        content: message.content.replace(pattern as RegExp, replacementFn as () => string),\n",
      "      }));\n",
      "    },\n",
      "  }));\n",
      "\n",
      "  for (const strategy of strategies) {\n",
      "    const reducedMessages = await strategy.apply();\n",
      "    const reducedTokenCount = countTotalTokens(reducedMessages, model);\n",
      "    const savedTokens = totalTokenCount - reducedTokenCount;\n",
      "    strategy.savedTokens = savedTokens;\n",
      "    strategy.description = `${savedTokens} tokens`;\n",
      "  }\n",
      "\n",
      "  strategies = strategies.filter((s) => (s.savedTokens ? s.savedTokens > 1 : false));\n",
      "\n",
      "  const maxPossibleSaving = strategies.map((x) => x.savedTokens ?? 0).reduce((prev, current) => prev + current);\n",
      "\n",
      "  if (maxPossibleSaving < tokenOverflowCount) {\n",
      "    window.showInformationMessage(\n",
      "      `If we applied every token reduction strategy available, you would still be ${\n",
      "        tokenOverflowCount - maxPossibleSaving\n",
      "      } over the limit of the '${model}' model. Please reduce the size of the content.`,\n",
      "      { modal: true }\n",
      "    );\n",
      "  }\n",
      "\n",
      "  const selectedStrategies = await window.showQuickPick(strategies, {\n",
      "    canPickMany: true,\n",
      "    title: uiText.tooManyTokens,\n",
      "    placeHolder: uiText.tooManyTokensPlaceholder,\n",
      "  });\n",
      "\n",
      "  if (!selectedStrategies) {\n",
      "    return null;\n",
      "  }\n",
      "\n",
      "  let reducedMessages = messages;\n",
      "  for (const strategy of selectedStrategies) {\n",
      "    reducedMessages = await strategy.apply(reducedMessages);\n",
      "  }\n",
      "\n",
      "  const reducedTokenCount = countTotalTokens(reducedMessages, model);\n",
      "  if (reducedTokenCount > limit) {\n",
      "    window.showErrorMessage(msgs.notEnoughSavings);\n",
      "    return null;\n",
      "  }\n",
      "\n",
      "  return reducedMessages;\n",
      "}\n",
      "\n",
      "export function countTokens(text: string, model: any): number {\n",
      "  const enc = encoding_for_model(model);\n",
      "  const tokenCount = enc.encode(text).length;\n",
      "  enc.free();\n",
      "  return tokenCount;\n",
      "}\n",
      "\n",
      "export function countTotalTokens(msgs: ChatCompletionRequestMessage[], model: string): number {\n",
      "  return msgs.reduce((accumulator, message) => {\n",
      "    return accumulator + countTokens(message.content, model);\n",
      "  }, 0);\n",
      "}\n",
      " \n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import List\n",
    "import re\n",
    "\n",
    "def print_file_content(file_path: Path):\n",
    "    content = file_path.read_text()\n",
    "\n",
    "    # Remove single-line comments\n",
    "    content = re.sub(r\"//.*\", \"\", content)\n",
    "\n",
    "    # Remove multi-line comments\n",
    "    content = re.sub(r\"/\\*[\\s\\S]*?\\*/\", \"\", content)\n",
    "\n",
    "    print(f\"\\n\\nFile: '{file_path}'\")\n",
    "    print(\"\\n\", content, \"\")\n",
    "\n",
    "def process_files(file_patterns: List[str]):\n",
    "    for pattern in file_patterns:\n",
    "        for file in Path().glob(pattern):\n",
    "            print_file_content(file)\n",
    "\n",
    "file_patterns = [\"../../README.md\", \"../../src/extension.ts\", \"../../src/completion.ts\", \"../../src/cellUtils.ts\", \"../../src/streamUtils.ts\", \"../../src/tokenUtils.ts\"]\n",
    "process_files(file_patterns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Review the VSCode extension above for potential bugs. You can infer the purpose of the extension by reading the README.md .**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "assistant"
    ]
   },
   "source": [
    "I have reviewed the code and found a few potential issues that you might want to address:\n",
    "\n",
    "1. In `src/cellUtils.ts`, the function `convertCellsToMessages` has a condition `if (systemMessages.length === 0 && totalLengthUserMessages > 32)`. The number `32` seems arbitrary and might cause issues if the user has a different requirement. Consider making this a configurable parameter.\n",
    "\n",
    "2. In `src/completion.ts`, the function `generateCompletion` has a condition `if (limit !== null && totalTokenCount > limit)`. If the limit is not set (i.e., `null`), the code will not check for token overflow. It's better to handle this case explicitly and provide a default limit or a proper error message.\n",
    "\n",
    "3. In `src/tokenUtils.ts`, the function `applyTokenReductions` uses a hardcoded list of token reduction strategies. It would be better to make this list configurable or extensible, so users can add their own strategies if needed.\n",
    "\n",
    "4. In `src/extension.ts`, the function `setParam` has a parameter `validateFn` which is expected to return a boolean value. However, in the case of the `setLogitBias` command, the `validateFn` returns a string or `null`. This inconsistency might cause issues in the future. Consider updating the `validateFn` for `setLogitBias` to return a boolean value.\n",
    "\n",
    "5. In `src/streamUtils.ts`, the function `bufferWholeChunks` has a complex condition to check if the value is a string and does not include specific characters. This condition might be hard to maintain and understand. Consider simplifying this condition or adding comments to explain its purpose."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "model": "gpt-4",
 "nbformat": 4,
 "nbformat_minor": 2
}
