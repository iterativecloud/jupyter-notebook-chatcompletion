{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "system"
    ]
   },
   "source": [
    "> You are a code generator that can only answer with python code."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File: 'example.ts'\n",
    "\n",
    "```typescript\n",
    "function greet(name: string) {\n",
    "  console.log(\"Hello, \" + name);\n",
    "}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add the age of the user to the console log.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": [
     "assistant"
    ]
   },
   "outputs": [],
   "source": [
    "with open('example.ts', 'w') as f: f.write(\"\"\"\n",
    "function greet(name: string, age : number) {\n",
    "  console.log(`Hello, ${name}. You are ${age} years old.`);\n",
    "}\n",
    "\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File: '../../src/finishReason.ts'\n",
    "\n",
    "```typescript\n",
    "export enum FinishReason {\n",
    "  length,\n",
    "  contentFilter,\n",
    "  stop,\n",
    "  null,\n",
    "  cancelled\n",
    "}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**My compiler says that FinishReason \"timeout\" is missing. Fix the issue.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "assistant"
    ]
   },
   "outputs": [],
   "source": [
    "with open('../../src/finishReason.ts', 'w') as f: f.write(\"\"\"\n",
    "export enum FinishReason {\n",
    "  length,\n",
    "  contentFilter,\n",
    "  stop,\n",
    "  null,\n",
    "  cancelled,\n",
    "  timeout\n",
    "}\n",
    "\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "File: '../../src/completion.ts'\n",
      "\n",
      " \n",
      "\n",
      "import axios from \"axios\";\n",
      "import { ChatCompletionRequestMessage, Configuration, CreateChatCompletionRequest, OpenAIApi } from \"openai\";\n",
      "import {\n",
      "  CancellationToken,\n",
      "  ConfigurationTarget,\n",
      "  NotebookCellKind,\n",
      "  NotebookEdit,\n",
      "  NotebookRange,\n",
      "  QuickPickItem,\n",
      "  WorkspaceEdit,\n",
      "  window,\n",
      "  workspace,\n",
      "} from \"vscode\";\n",
      "import { appendTextToCell, convertCellsToMessages, insertCell } from \"./cellUtils\";\n",
      "import { CompletionType } from \"./completionType\";\n",
      "import { FinishReason } from \"./finishReason\";\n",
      "import { bufferWholeChunks, streamChatCompletion } from \"./streamUtils\";\n",
      "import { UIProgress } from \"./uiProgress\";\n",
      "import { encoding_for_model } from \"@dqbd/tiktoken\";\n",
      "import { configKeys, msgs, uiText } from \"./constants\";\n",
      "\n",
      "const output = window.createOutputChannel(\"Notebook ChatCompletion\");\n",
      "type TokenReductionStrategy = QuickPickItem & {\n",
      "  apply: Function;\n",
      "  savedTokens?: number;\n",
      "};\n",
      "\n",
      "export async function generateCompletion(\n",
      "  cellIndex: number,\n",
      "  completionType: CompletionType,\n",
      "  progress: UIProgress,\n",
      "  cancelToken: CancellationToken\n",
      "): Promise<FinishReason> {\n",
      "  const e = window.activeNotebookEditor!;\n",
      "  let messages = await convertCellsToMessages(cellIndex, completionType);\n",
      "  let ck: NotebookCellKind | undefined = undefined;\n",
      "\n",
      "  const openaiApiKey = await getOpenAIApiKey();\n",
      "\n",
      "  if (!openaiApiKey) {\n",
      "    throw new Error(msgs.apiKeyNotSet);\n",
      "  }\n",
      "\n",
      "  const openai = new OpenAIApi(new Configuration({ apiKey: openaiApiKey }));\n",
      "\n",
      "  const tokenSource = axios.CancelToken.source();\n",
      "  cancelToken.onCancellationRequested(tokenSource.cancel);\n",
      "\n",
      "  const nbMetadata = e.notebook.metadata.custom;\n",
      "\n",
      "  const defaultModel = workspace.getConfiguration().get<string>(\"notebook-chatcompletion.defaultModel\");\n",
      "\n",
      "  const model = nbMetadata?.model ?? defaultModel;\n",
      "  const temperature = nbMetadata?.temperature ?? 0;\n",
      "\n",
      "  const limit = getTokenLimit(model);\n",
      "\n",
      "  const msgText = JSON.stringify(messages);\n",
      "  const totalTokenCount = countTokens(msgText, model);\n",
      "\n",
      "  if (limit !== null && totalTokenCount > limit) {\n",
      "    const tokenOverflow = limit - totalTokenCount;\n",
      "\n",
      "    const msgText = messages.map((x) => x.content).join();\n",
      "    const contentTokenCount = countTokens(msgText, model);\n",
      "\n",
      "    const reducedMessages = await applyTokenReductionStrategies(messages, tokenOverflow, contentTokenCount, limit, model);\n",
      "\n",
      "    if (!reducedMessages) {\n",
      "      return FinishReason.cancelled;\n",
      "    }\n",
      "\n",
      "    messages = reducedMessages;\n",
      "  }\n",
      "\n",
      "  let reqParams: CreateChatCompletionRequest = {\n",
      "    model: model,\n",
      "    messages: messages,\n",
      "    stream: true,\n",
      "    temperature: temperature,\n",
      "  };\n",
      "\n",
      "  if (limit) {\n",
      "    const reducedMsgText = JSON.stringify(messages);\n",
      "    const reducedTokenCount = countTokens(reducedMsgText, model);\n",
      "    reqParams.max_tokens = limit - reducedTokenCount;\n",
      "\n",
      "    if (reqParams.max_tokens < 1) {\n",
      "      const result = await window.showInformationMessage(\n",
      "        `The request is estimated to be ${-reqParams.max_tokens} tokens over the limit (including the input) and will likely be rejected from the OpenAI API. Do you still want to proceed?`,\n",
      "        { modal: true },\n",
      "        \"Yes\"\n",
      "      );\n",
      "      if (result !== \"Yes\") {\n",
      "        return FinishReason.cancelled;\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "\n",
      "  reqParams = addParametersFromMetadata(nbMetadata, reqParams);\n",
      "\n",
      "  output.appendLine(\"\\n\" + JSON.stringify(reqParams, undefined, 2) + \"\\n\");\n",
      "  progress.report({ increment: 1, message: msgs.sendingRequest });\n",
      "\n",
      "  const response = await openai.createChatCompletion(reqParams, {\n",
      "    cancelToken: tokenSource.token,\n",
      "    responseType: \"stream\",\n",
      "  });\n",
      "\n",
      "  for await (let textToken of bufferWholeChunks(streamChatCompletion(response, cancelToken))) {\n",
      "    if (Object.values(FinishReason).includes(textToken as FinishReason)) {\n",
      "      switch (textToken) {\n",
      "        case FinishReason.length:\n",
      "          output.append(\"FINISH_REASON_LENGTH\" + \"\\n\");\n",
      "          break;\n",
      "        case FinishReason.contentFilter:\n",
      "          output.append(\"FINISH_REASON_CONTENTFILTER\" + \"\\n\");\n",
      "          break;\n",
      "        case FinishReason.stop:\n",
      "          output.append(\"FINISH_REASON_STOP\" + \"\\n\");\n",
      "          break;\n",
      "      }\n",
      "\n",
      "      const currentCell = e.notebook.cellAt(cellIndex);\n",
      "      const text = currentCell.document.getText();\n",
      "\n",
      "      if (!/\\S/.test(text)) {\n",
      "        const edit = new WorkspaceEdit();\n",
      "        edit.set(currentCell.notebook.uri, [NotebookEdit.deleteCells(new NotebookRange(currentCell.index, currentCell.index + 1))]);\n",
      "        await workspace.applyEdit(edit);\n",
      "      }\n",
      "\n",
      "      return textToken as FinishReason;\n",
      "    } else {\n",
      "      output.append(textToken.toString());\n",
      "    }\n",
      "\n",
      "    if (typeof textToken !== \"string\") {\n",
      "      throw new Error(`Unknown stream result: ${textToken}`);\n",
      "    }\n",
      "\n",
      "    if (textToken.includes(\"```python\\n\")) {\n",
      "      ck = NotebookCellKind.Code;\n",
      "\n",
      "      cellIndex = await insertCell(e, cellIndex, ck, \"python\");\n",
      "      textToken = textToken.replace(\"```python\\n\", \"\");\n",
      "    } else if (textToken.includes(\"```\") && ck === NotebookCellKind.Code) {\n",
      "      textToken = textToken.replace(\"```\", \"\");\n",
      "\n",
      "      ck = NotebookCellKind.Markup;\n",
      "      cellIndex = await insertCell(e, cellIndex, ck);\n",
      "    }\n",
      "\n",
      "    if (ck === undefined) {\n",
      "      cellIndex = await insertCell(e, cellIndex, NotebookCellKind.Markup);\n",
      "      ck = NotebookCellKind.Markup;\n",
      "    }\n",
      "\n",
      "    await appendTextToCell(e, cellIndex, textToken);\n",
      "\n",
      "    progress.report({ increment: 0.5, message: msgs.receivingTokens });\n",
      "  }\n",
      "\n",
      "  return FinishReason.length;\n",
      "}\n",
      "\n",
      "function addParametersFromMetadata(nbMetadata: any, reqParams: CreateChatCompletionRequest) {\n",
      "  const e = window.activeNotebookEditor;\n",
      "  if (e && nbMetadata) {\n",
      "    if (e.notebook.metadata.custom?.top_p) {\n",
      "      reqParams = {\n",
      "        ...reqParams,\n",
      "        top_p: e.notebook.metadata.custom.top_p,\n",
      "      };\n",
      "    }\n",
      "    if (e.notebook.metadata.custom?.n) {\n",
      "      reqParams = {\n",
      "        ...reqParams,\n",
      "        n: e.notebook.metadata.custom.n,\n",
      "      };\n",
      "    }\n",
      "    if (e.notebook.metadata.custom?.max_tokens) {\n",
      "      reqParams.max_tokens = e.notebook.metadata.custom.max_tokens;\n",
      "    }\n",
      "    if (e.notebook.metadata.custom?.presence_penalty) {\n",
      "      reqParams = {\n",
      "        ...reqParams,\n",
      "        presence_penalty: e.notebook.metadata.custom.presence_penalty,\n",
      "      };\n",
      "    }\n",
      "    if (e.notebook.metadata.custom?.frequency_penalty) {\n",
      "      reqParams = {\n",
      "        ...reqParams,\n",
      "        frequency_penalty: e.notebook.metadata.custom.frequency_penalty,\n",
      "      };\n",
      "    }\n",
      "    if (e.notebook.metadata.custom?.logit_bias) {\n",
      "      reqParams = {\n",
      "        ...reqParams,\n",
      "        logit_bias: e.notebook.metadata.custom.logit_bias,\n",
      "      };\n",
      "    }\n",
      "    if (e.notebook.metadata.custom?.user) {\n",
      "      reqParams = {\n",
      "        ...reqParams,\n",
      "        user: e.notebook.metadata.custom.top_p,\n",
      "      };\n",
      "    }\n",
      "  }\n",
      "  return reqParams;\n",
      "}\n",
      "\n",
      "async function getOpenAIApiKey(): Promise<string> {\n",
      "  let apiKey = workspace.getConfiguration().get<string>(configKeys.openAiKey);\n",
      "  if (!apiKey) {\n",
      "    apiKey = await window.showInputBox({\n",
      "      prompt: msgs.enterApiKey,\n",
      "      validateInput: (value) => (value.trim().length > 0 ? null : msgs.apiKeyCannotBeEmpty),\n",
      "    });\n",
      "\n",
      "    if (apiKey) {\n",
      "      await workspace.getConfiguration().update(configKeys.openAiKey, apiKey, ConfigurationTarget.Global);\n",
      "\n",
      "      await window.showInformationMessage(msgs.modelNotAccessible, { modal: true });\n",
      "    } else {\n",
      "      window.showErrorMessage(msgs.apiKeyRequired, { modal: true });\n",
      "      return \"\";\n",
      "    }\n",
      "  }\n",
      "\n",
      "  return apiKey;\n",
      "}\n",
      "\n",
      "function getTokenLimit(model: string): number | null {\n",
      "  let limit: number | null = null;\n",
      "\n",
      "  switch (model) {\n",
      "    case \"gpt-4\":\n",
      "    case \"gpt-4-0314\":\n",
      "      limit = 8192;\n",
      "      break;\n",
      "\n",
      "    case \"gpt-4-32k\":\n",
      "    case \"gpt-4-32k-0314\":\n",
      "      limit = 32768;\n",
      "      break;\n",
      "\n",
      "    case \"gpt-3.5-turbo\":\n",
      "    case \"gpt-3.5-turbo-0301\":\n",
      "      limit = 4096;\n",
      "      break;\n",
      "\n",
      "    default:\n",
      "      break;\n",
      "  }\n",
      "\n",
      "  return limit;\n",
      "}\n",
      "\n",
      "async function applyTokenReductionStrategies(\n",
      "  messages: ChatCompletionRequestMessage[],\n",
      "  tokenOverflowCount: number,\n",
      "  totalTokenCount: number,\n",
      "  limit: number,\n",
      "  model: string\n",
      "): Promise<ChatCompletionRequestMessage[] | null> {\n",
      "  let strategies: TokenReductionStrategy[] = [\n",
      "    {\n",
      "      label: uiText.removeOutput,\n",
      "      apply: async () => {\n",
      "        return messages.filter((message) => !message.content.startsWith(\"Output from previous code:\"));\n",
      "      },\n",
      "    },\n",
      "    {\n",
      "      label: uiText.removeProblems,\n",
      "      apply: async () => {\n",
      "        return messages.filter((message) => !message.content.startsWith(\"Problems reported by VSCode from previous code:\"));\n",
      "      },\n",
      "    },\n",
      "    {\n",
      "      label: uiText.removeSpaces,\n",
      "      apply: async () => {\n",
      "        return messages.map((message) => ({\n",
      "          ...message,\n",
      "          content: message.content.replace(/ /g, \"\"),\n",
      "        }));\n",
      "      },\n",
      "    },\n",
      "    {\n",
      "      label: uiText.removeLineBreaks,\n",
      "      apply: async () => {\n",
      "        return messages.map((message) => ({\n",
      "          ...message,\n",
      "          content: message.content.replace(/\\n/g, \"\"),\n",
      "        }));\n",
      "      },\n",
      "    },\n",
      "    {\n",
      "      label: uiText.removePunctuation,\n",
      "      apply: async () => {\n",
      "        return messages.map((message) => ({\n",
      "          ...message,\n",
      "          content: message.content.replace(/[.,;:!?]/g, \"\"),\n",
      "        }));\n",
      "      },\n",
      "    },\n",
      "  ];\n",
      "\n",
      "  for (const strategy of strategies) {\n",
      "    const reducedMessages = await strategy.apply();\n",
      "    const reducedTokenCount = countTotalTokens(reducedMessages, model);\n",
      "    const savedTokens = totalTokenCount - reducedTokenCount;\n",
      "    strategy.savedTokens = savedTokens;\n",
      "    strategy.description = `${savedTokens} tokens`;\n",
      "  }\n",
      "\n",
      "  strategies = strategies.filter((s) => (s.savedTokens ? s.savedTokens > 1 : false));\n",
      "\n",
      "  const maxPossibleSaving = strategies.map((x) => x.savedTokens ?? 0).reduce((prev, current) => prev + current);\n",
      "\n",
      "  if (maxPossibleSaving < tokenOverflowCount) {\n",
      "    window.showInformationMessage(\n",
      "      `If we applied every token reduction strategy available, you would still be ${\n",
      "        tokenOverflowCount - maxPossibleSaving\n",
      "      } over the limit of the '${model}' model. Please reduce the size of the content.`,\n",
      "      { modal: true }\n",
      "    );\n",
      "  }\n",
      "\n",
      "  const selectedStrategies = await window.showQuickPick(strategies, {\n",
      "    canPickMany: true,\n",
      "    title: uiText.tooManyTokens,\n",
      "    placeHolder: uiText.tooManyTokensPlaceholder,\n",
      "  });\n",
      "\n",
      "  if (!selectedStrategies) {\n",
      "    return null;\n",
      "  }\n",
      "\n",
      "  let reducedMessages = messages;\n",
      "  for (const strategy of selectedStrategies) {\n",
      "    reducedMessages = await strategy.apply(reducedMessages);\n",
      "  }\n",
      "\n",
      "  const reducedTokenCount = countTotalTokens(reducedMessages, model);\n",
      "  if (reducedTokenCount > limit) {\n",
      "    window.showErrorMessage(msgs.notEnoughSavings);\n",
      "    return null;\n",
      "  }\n",
      "\n",
      "  return reducedMessages;\n",
      "}\n",
      "\n",
      "function countTokens(text: string, model: any): number {\n",
      "  const enc = encoding_for_model(model);\n",
      "  const tokenCount = enc.encode(text).length;\n",
      "  enc.free();\n",
      "  return tokenCount;\n",
      "}\n",
      "\n",
      "function countTotalTokens(msgs: ChatCompletionRequestMessage[], model: string): number {\n",
      "  return msgs.reduce((accumulator, message) => {\n",
      "    return accumulator + countTokens(message.content, model);\n",
      "  }, 0);\n",
      "}\n",
      " \n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import List\n",
    "import re\n",
    "\n",
    "def print_file_content(file_path: Path):\n",
    "    content = file_path.read_text()\n",
    "\n",
    "    # Remove single-line comments\n",
    "    content = re.sub(r\"//.*\", \"\", content)\n",
    "\n",
    "    # Remove multi-line comments\n",
    "    content = re.sub(r\"/\\*[\\s\\S]*?\\*/\", \"\", content)\n",
    "\n",
    "    print(f\"\\n\\nFile: '{file_path}'\")\n",
    "    print(\"\\n\", content, \"\")\n",
    "\n",
    "def process_files(file_patterns: List[str]):\n",
    "    for pattern in file_patterns:\n",
    "        for file in Path().glob(pattern):\n",
    "            print_file_content(file)\n",
    "\n",
    "file_patterns = [\"../../src/completion.ts\"]\n",
    "process_files(file_patterns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In completion.ts, the applyTokenReductionStrategies and addParametersFromMetadata have a lot of repetition. Please devise a plan on how to reduce those repetitions. Apply the changes from your plan like you did in Example 1 and 2.**\n",
    "**Don't forget to espace \"\\n\" with \"\\\\n\" in the code, otherwise it will be written as an actual line-break."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "assistant"
    ]
   },
   "source": [
    "To reduce repetition in the `applyTokenReductionStrategies` function, we can create a list of tuples containing the label, the regex pattern to replace, and the replacement string. Then, we can loop through the list and apply the replacements using a single function. This will make the code more concise and easier to maintain.\n",
    "\n",
    "For the `addParametersFromMetadata` function, we can create a dictionary that maps the metadata keys to the corresponding request parameter keys. Then, we can loop through the dictionary and update the request parameters if the metadata key exists.\n",
    "\n",
    "Here's the updated code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "assistant"
    ]
   },
   "outputs": [],
   "source": [
    "with open('../../src/completion.ts', 'w') as f: f.write(\"\"\"\n",
    "import axios from \"axios\";\n",
    "import { ChatCompletionRequestMessage, Configuration, CreateChatCompletionRequest, OpenAIApi } from \"openai\";\n",
    "import {\n",
    "  CancellationToken,\n",
    "  ConfigurationTarget,\n",
    "  NotebookCellKind,\n",
    "  NotebookEdit,\n",
    "  NotebookRange,\n",
    "  QuickPickItem,\n",
    "  WorkspaceEdit,\n",
    "  window,\n",
    "  workspace,\n",
    "} from \"vscode\";\n",
    "import { appendTextToCell, convertCellsToMessages, insertCell } from \"./cellUtils\";\n",
    "import { CompletionType } from \"./completionType\";\n",
    "import { FinishReason } from \"./finishReason\";\n",
    "import { bufferWholeChunks, streamChatCompletion } from \"./streamUtils\";\n",
    "import { UIProgress } from \"./uiProgress\";\n",
    "import { encoding_for_model } from \"@dqbd/tiktoken\";\n",
    "import { configKeys, msgs, uiText } from \"./constants\";\n",
    "\n",
    "const output = window.createOutputChannel(\"Notebook ChatCompletion\");\n",
    "type TokenReductionStrategy = QuickPickItem & {\n",
    "  apply: Function;\n",
    "  savedTokens?: number;\n",
    "};\n",
    "\n",
    "export async function generateCompletion(\n",
    "  cellIndex: number,\n",
    "  completionType: CompletionType,\n",
    "  progress: UIProgress,\n",
    "  cancelToken: CancellationToken\n",
    "): Promise<FinishReason> {\n",
    "  const e = window.activeNotebookEditor!;\n",
    "  let messages = await convertCellsToMessages(cellIndex, completionType);\n",
    "  let ck: NotebookCellKind | undefined = undefined;\n",
    "\n",
    "  const openaiApiKey = await getOpenAIApiKey();\n",
    "\n",
    "  if (!openaiApiKey) {\n",
    "    throw new Error(msgs.apiKeyNotSet);\n",
    "  }\n",
    "\n",
    "  const openai = new OpenAIApi(new Configuration({ apiKey: openaiApiKey }));\n",
    "\n",
    "  const tokenSource = axios.CancelToken.source();\n",
    "  cancelToken.onCancellationRequested(tokenSource.cancel);\n",
    "\n",
    "  const nbMetadata = e.notebook.metadata.custom;\n",
    "\n",
    "  const defaultModel = workspace.getConfiguration().get<string>(\"notebook-chatcompletion.defaultModel\");\n",
    "\n",
    "  const model = nbMetadata?.model ?? defaultModel;\n",
    "  const temperature = nbMetadata?.temperature ?? 0;\n",
    "\n",
    "  const limit = getTokenLimit(model);\n",
    "\n",
    "  const msgText = JSON.stringify(messages);\n",
    "  const totalTokenCount = countTokens(msgText, model);\n",
    "\n",
    "  if (limit !== null && totalTokenCount > limit) {\n",
    "    const tokenOverflow = limit - totalTokenCount;\n",
    "\n",
    "    const msgText = messages.map((x) => x.content).join();\n",
    "    const contentTokenCount = countTokens(msgText, model);\n",
    "\n",
    "    const reducedMessages = await applyTokenReductionStrategies(messages, tokenOverflow, contentTokenCount, limit, model);\n",
    "\n",
    "    if (!reducedMessages) {\n",
    "      return FinishReason.cancelled;\n",
    "    }\n",
    "\n",
    "    messages = reducedMessages;\n",
    "  }\n",
    "\n",
    "  let reqParams: CreateChatCompletionRequest = {\n",
    "    model: model,\n",
    "    messages: messages,\n",
    "    stream: true,\n",
    "    temperature: temperature,\n",
    "  };\n",
    "\n",
    "  if (limit) {\n",
    "    const reducedMsgText = JSON.stringify(messages);\n",
    "    const reducedTokenCount = countTokens(reducedMsgText, model);\n",
    "    reqParams.max_tokens = limit - reducedTokenCount;\n",
    "\n",
    "    if (reqParams.max_tokens < 1) {\n",
    "      const result = await window.showInformationMessage(\n",
    "        `The request is estimated to be ${-reqParams.max_tokens} tokens over the limit (including the input) and will likely be rejected from the OpenAI API. Do you still want to proceed?`,\n",
    "        { modal: true },\n",
    "        \"Yes\"\n",
    "      );\n",
    "      if (result !== \"Yes\") {\n",
    "        return FinishReason.cancelled;\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "\n",
    "  reqParams = addParametersFromMetadata(nbMetadata, reqParams);\n",
    "\n",
    "  output.appendLine(\"\\\\n\" + JSON.stringify(reqParams, undefined, 2) + \"\\\\n\");\n",
    "  progress.report({ increment: 1, message: msgs.sendingRequest });\n",
    "\n",
    "  const response = await openai.createChatCompletion(reqParams, {\n",
    "    cancelToken: tokenSource.token,\n",
    "    responseType: \"stream\",\n",
    "  });\n",
    "\n",
    "  for await (let textToken of bufferWholeChunks(streamChatCompletion(response, cancelToken))) {\n",
    "    if (Object.values(FinishReason).includes(textToken as FinishReason)) {\n",
    "      switch (textToken) {\n",
    "        case FinishReason.length:\n",
    "          output.append(\"FINISH_REASON_LENGTH\" + \"\\\\n\");\n",
    "          break;\n",
    "        case FinishReason.contentFilter:\n",
    "          output.append(\"FINISH_REASON_CONTENTFILTER\" + \"\\\\n\");\n",
    "          break;\n",
    "        case FinishReason.stop:\n",
    "          output.append(\"FINISH_REASON_STOP\" + \"\\\\n\");\n",
    "          break;\n",
    "      }\n",
    "\n",
    "      const currentCell = e.notebook.cellAt(cellIndex);\n",
    "      const text = currentCell.document.getText();\n",
    "\n",
    "      if (!/\\\\S/.test(text)) {\n",
    "        const edit = new WorkspaceEdit();\n",
    "        edit.set(currentCell.notebook.uri, [NotebookEdit.deleteCells(new NotebookRange(currentCell.index, currentCell.index + 1))]);\n",
    "        await workspace.applyEdit(edit);\n",
    "      }\n",
    "\n",
    "      return textToken as FinishReason;\n",
    "    } else {\n",
    "      output.append(textToken.toString());\n",
    "    }\n",
    "\n",
    "    if (typeof textToken !== \"string\") {\n",
    "      throw new Error(`Unknown stream result: ${textToken}`);\n",
    "    }\n",
    "\n",
    "    if (textToken.includes(\"```python\\\\n\")) {\n",
    "      ck = NotebookCellKind.Code;\n",
    "\n",
    "      cellIndex = await insertCell(e, cellIndex, ck, \"python\");\n",
    "      textToken = textToken.replace(\"```python\\\\n\", \"\");\n",
    "    } else if (textToken.includes(\"```\") && ck === NotebookCellKind.Code) {\n",
    "      textToken = textToken.replace(\"```\", \"\");\n",
    "\n",
    "      ck = NotebookCellKind.Markup;\n",
    "      cellIndex = await insertCell(e, cellIndex, ck);\n",
    "    }\n",
    "\n",
    "    if (ck === undefined) {\n",
    "      cellIndex = await insertCell(e, cellIndex, NotebookCellKind.Markup);\n",
    "      ck = NotebookCellKind.Markup;\n",
    "    }\n",
    "\n",
    "    await appendTextToCell(e, cellIndex, textToken);\n",
    "\n",
    "    progress.report({ increment: 0.5, message: msgs.receivingTokens });\n",
    "  }\n",
    "\n",
    "  return FinishReason.length;\n",
    "}\n",
    "\n",
    "function addParametersFromMetadata(nbMetadata: any, reqParams: CreateChatCompletionRequest) {\n",
    "  const metadataToReqParamMap = {\n",
    "    top_p: \"top_p\",\n",
    "    n: \"n\",\n",
    "    max_tokens: \"max_tokens\",\n",
    "    presence_penalty: \"presence_penalty\",\n",
    "    frequency_penalty: \"frequency_penalty\",\n",
    "    logit_bias: \"logit_bias\",\n",
    "    user: \"user\",\n",
    "  };\n",
    "\n",
    "  for (const [metadataKey, reqParamKey] of Object.entries(metadataToReqParamMap)) {\n",
    "    if (nbMetadata && nbMetadata[metadataKey]) {\n",
    "      reqParams[reqParamKey] = nbMetadata[metadataKey];\n",
    "    }\n",
    "  }\n",
    "\n",
    "  return reqParams;\n",
    "}\n",
    "\n",
    "async function getOpenAIApiKey(): Promise<string> {\n",
    "  let apiKey = workspace.getConfiguration().get<string>(configKeys.openAiKey);\n",
    "  if (!apiKey) {\n",
    "    apiKey = await window.showInputBox({\n",
    "      prompt: msgs.enterApiKey,\n",
    "      validateInput: (value) => (value.trim().length > 0 ? null : msgs.apiKeyCannotBeEmpty),\n",
    "    });\n",
    "\n",
    "    if (apiKey) {\n",
    "      await workspace.getConfiguration().update(configKeys.openAiKey, apiKey, ConfigurationTarget.Global);\n",
    "\n",
    "      await window.showInformationMessage(msgs.modelNotAccessible, { modal: true });\n",
    "    } else {\n",
    "      window.showErrorMessage(msgs.apiKeyRequired, { modal: true });\n",
    "      return \"\";\n",
    "    }\n",
    "  }\n",
    "\n",
    "  return apiKey;\n",
    "}\n",
    "\n",
    "function getTokenLimit(model: string): number | null {\n",
    "  let limit: number | null = null;\n",
    "\n",
    "  switch (model) {\n",
    "    case \"gpt-4\":\n",
    "    case \"gpt-4-0314\":\n",
    "      limit = 8192;\n",
    "      break;\n",
    "\n",
    "    case \"gpt-4-32k\":\n",
    "    case \"gpt-4-32k-0314\":\n",
    "      limit = 32768;\n",
    "      break;\n",
    "\n",
    "    case \"gpt-3.5-turbo\":\n",
    "    case \"gpt-3.5-turbo-0301\":\n",
    "      limit = 4096;\n",
    "      break;\n",
    "\n",
    "    default:\n",
    "      break;\n",
    "  }\n",
    "\n",
    "  return limit;\n",
    "}\n",
    "\n",
    "async function applyTokenReductionStrategies(\n",
    "  messages: ChatCompletionRequestMessage[],\n",
    "  tokenOverflowCount: number,\n",
    "  totalTokenCount: number,\n",
    "  limit: number,\n",
    "  model: string\n",
    "): Promise<ChatCompletionRequestMessage[] | null> {\n",
    "  const replacements = [\n",
    "    [uiText.removeOutput, /^Output from previous code:.*\\\\n?/gm, \"\"],\n",
    "    [uiText.removeProblems, /^Problems reported by VSCode from previous code:.*\\\\n?/gm, \"\"],\n",
    "    [uiText.removeSpaces, / /g, \"\"],\n",
    "    [uiText.removeLineBreaks, /\\\\n/g, \"\"],\n",
    "    [uiText.removePunctuation, /[.,;:!?]/g, \"\"],\n",
    "  ];\n",
    "\n",
    "  let strategies: TokenReductionStrategy[] = replacements.map(([label, pattern, replacement]) => ({\n",
    "    label,\n",
    "    apply: async () => {\n",
    "      return messages.map((message) => ({\n",
    "        ...message,\n",
    "        content: message.content.replace(pattern, replacement),\n",
    "      }));\n",
    "    },\n",
    "  }));\n",
    "\n",
    "  for (const strategy of strategies) {\n",
    "    const reducedMessages = await strategy.apply();\n",
    "    const reducedTokenCount = countTotalTokens(reducedMessages, model);\n",
    "    const savedTokens = totalTokenCount - reducedTokenCount;\n",
    "    strategy.savedTokens = savedTokens;\n",
    "    strategy.description = `${savedTokens} tokens`;\n",
    "  }\n",
    "\n",
    "  strategies = strategies.filter((s) => (s.savedTokens ? s.savedTokens > 1 : false));\n",
    "\n",
    "  const maxPossibleSaving = strategies.map((x) => x.savedTokens ?? 0).reduce((prev, current) => prev + current);\n",
    "\n",
    "  if (maxPossibleSaving < tokenOverflowCount) {\n",
    "    window.showInformationMessage(\n",
    "      `If we applied every token reduction strategy available, you would still be ${\n",
    "        tokenOverflowCount - maxPossibleSaving\n",
    "      } over the limit of the '${model}' model. Please reduce the size of the content.`,\n",
    "      { modal: true }\n",
    "    );\n",
    "  }\n",
    "\n",
    "  const selectedStrategies = await window.showQuickPick(strategies, {\n",
    "    canPickMany: true,\n",
    "    title: uiText.tooManyTokens,\n",
    "    placeHolder: uiText.tooManyTokensPlaceholder,\n",
    "  });\n",
    "\n",
    "  if (!selectedStrategies) {\n",
    "    return null;\n",
    "  }\n",
    "\n",
    "  let reducedMessages = messages;\n",
    "  for (const strategy of selectedStrategies) {\n",
    "    reducedMessages = await strategy.apply(reducedMessages);\n",
    "  }\n",
    "\n",
    "  const reducedTokenCount = countTotalTokens(reducedMessages, model);\n",
    "  if (reducedTokenCount > limit) {\n",
    "    window.showErrorMessage(msgs.notEnoughSavings);\n",
    "    return null;\n",
    "  }\n",
    "\n",
    "  return reducedMessages;\n",
    "}\n",
    "\n",
    "function countTokens(text: string, model: any): number {\n",
    "  const enc = encoding_for_model(model);\n",
    "  const tokenCount = enc.encode(text).length;\n",
    "  enc.free();\n",
    "  return tokenCount;\n",
    "}\n",
    "\n",
    "function countTotalTokens(msgs: ChatCompletionRequestMessage[], model: string): number {\n",
    "  return msgs.reduce((accumulator, message) => {\n",
    "    return accumulator + countTokens(message.content, model);\n",
    "  }, 0);\n",
    "}\n",
    "\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I get the following issue for the line with reqParams[reqParamKey] = nbMetadata[metadataKey];\n",
    "Element implicitly has an 'any' type because expression of type 'string' can't be used to index type 'CreateChatCompletionRequest'.\n",
    "  No index signature with a parameter of type 'string' was found on type 'CreateChatCompletionRequest'."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "assistant"
    ]
   },
   "source": [
    "To fix this issue, we can add an index signature to the `CreateChatCompletionRequest` type. However, since we don't have direct access to modify the `CreateChatCompletionRequest` type, we can create a new type that extends it and adds the index signature. Here's the updated code:\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wait, I have a request: continue, but this time directly show me the changed code.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "assistant"
    ]
   },
   "source": [
    "Here's the changed code to fix the issue:\n",
    "\n",
    "```typescript\n",
    "type ExtendedCreateChatCompletionRequest = CreateChatCompletionRequest & {\n",
    "  [key: string]: any;\n",
    "};\n",
    "\n",
    "function addParametersFromMetadata(nbMetadata: any, reqParams: CreateChatCompletionRequest) {\n",
    "  const metadataToReqParamMap = {\n",
    "    top_p: \"top_p\",\n",
    "    n: \"n\",\n",
    "    max_tokens: \"max_tokens\",\n",
    "    presence_penalty: \"presence_penalty\",\n",
    "    frequency_penalty: \"frequency_penalty\",\n",
    "    logit_bias: \"logit_bias\",\n",
    "    user: \"user\",\n",
    "  };\n",
    "\n",
    "  const extendedReqParams: ExtendedCreateChatCompletionRequest = reqParams;\n",
    "\n",
    "  for (const [metadataKey, reqParamKey] of Object.entries(metadataToReqParamMap)) {\n",
    "    if (nbMetadata && nbMetadata[metadataKey]) {\n",
    "      extendedReqParams[reqParamKey] = nbMetadata[metadataKey];\n",
    "    }\n",
    "  }\n",
    "\n",
    "  return extendedReqParams;\n",
    "}\n",
    "```\n",
    "\n",
    "Now, the `addParametersFromMetadata` function uses the `ExtendedCreateChatCompletionRequest` type, which allows indexing with a string key."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ok, that fixed the previous issue. I have one last issue on the \"replacement\" parameter for the line content: message.content.replace(pattern, replacement),**\n",
    "[{\n",
    "\t\"resource\": \"/home/minium/jupyter-notebook-chatcompletion-1/src/completion.ts\",\n",
    "\t\"owner\": \"typescript\",\n",
    "\t\"code\": \"2769\",\n",
    "\t\"severity\": 8,\n",
    "\t\"message\": \"No overload matches this call.\\n  The last overload gave the following error.\\n    Argument of type 'string | RegExp' is not assignable to parameter of type '(substring: string, ...args: any[]) => string'.\\n      Type 'string' is not assignable to type '(substring: string, ...args: any[]) => string'.\",\n",
    "\t\"source\": \"ts\",\n",
    "\t\"startLineNumber\": 261,\n",
    "\t\"startColumn\": 51,\n",
    "\t\"endLineNumber\": 261,\n",
    "\t\"endColumn\": 62,\n",
    "\t\"relatedInformation\": [\n",
    "\t\t{\n",
    "\t\t\t\"startLineNumber\": 457,\n",
    "\t\t\t\"startColumn\": 5,\n",
    "\t\t\t\"endLineNumber\": 457,\n",
    "\t\t\t\"endColumn\": 108,\n",
    "\t\t\t\"message\": \"The last overload is declared here.\",\n",
    "\t\t\t\"resource\": \"/home/minium/.vscode-server/bin/b3e4e68a0bc097f0ae7907b217c1119af9e03435/extensions/node_modules/typescript/lib/lib.es5.d.ts\"\n",
    "\t\t}\n",
    "\t]\n",
    "}]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "assistant"
    ]
   },
   "source": [
    "I apologize for the confusion. The issue is caused by the `replace` function expecting a function as the second argument when the first argument is a RegExp. To fix this issue, we can change the `replacements` list to contain functions instead of strings for the replacement values. Here's the updated code:\n",
    "\n",
    "```typescript\n",
    "const replacements = [\n",
    "  [uiText.removeOutput, /^Output from previous code:.*\\n?/gm, () => \"\"],\n",
    "  [uiText.removeProblems, /^Problems reported by VSCode from previous code:.*\\n?/gm, () => \"\"],\n",
    "  [uiText.removeSpaces, / /g, () => \"\"],\n",
    "  [uiText.removeLineBreaks, /\\n/g, () => \"\"],\n",
    "  [uiText.removePunctuation, /[.,;:!?]/g, () => \"\"],\n",
    "];\n",
    "\n",
    "let strategies: TokenReductionStrategy[] = replacements.map(([label, pattern, replacementFn]) => ({\n",
    "  label,\n",
    "  apply: async () => {\n",
    "    return messages.map((message) => ({\n",
    "      ...message,\n",
    "      content: message.content.replace(pattern, replacementFn),\n",
    "    }));\n",
    "  },\n",
    "}));\n",
    "```\n",
    "\n",
    "Now, the `replace` function should work correctly with both RegExp and string patterns."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I have these issues:\n",
    "Type '{ label: string | RegExp | (() => string); apply: () => Promise<{ content: string; role: ChatCompletionRequestMessageRoleEnum; name?: string | undefined; }[]>; }[]' is not assignable to type 'TokenReductionStrategy[]'.\n",
    "  Type '{ label: string | RegExp | (() => string); apply: () => Promise<{ content: string; role: ChatCompletionRequestMessageRoleEnum; name?: string | undefined; }[]>; }' is not assignable to type 'TokenReductionStrategy'.\n",
    "    Type '{ label: string | RegExp | (() => string); apply: () => Promise<{ content: string; role: ChatCompletionRequestMessageRoleEnum; name?: string | undefined; }[]>; }' is not assignable to type 'QuickPickItem'.\n",
    "      Types of property 'label' are incompatible.\n",
    "        Type 'string | RegExp | (() => string)' is not assignable to type 'string'.\n",
    "          Type 'RegExp' is not assignable to type 'string'."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "assistant"
    ]
   },
   "source": [
    "I apologize for the oversight. The issue is caused by the `label` property in the `strategies` list being assigned a value from the `replacements` list, which includes RegExp and functions. To fix this issue, we can separate the label from the pattern and replacement function in the `replacements` list. Here's the updated code:\n",
    "\n",
    "```typescript\n",
    "const replacements = [\n",
    "  [uiText.removeOutput, /^Output from previous code:.*\\n?/gm, () => \"\"],\n",
    "  [uiText.removeProblems, /^Problems reported by VSCode from previous code:.*\\n?/gm, () => \"\"],\n",
    "  [uiText.removeSpaces, / /g, () => \"\"],\n",
    "  [uiText.removeLineBreaks, /\\n/g, () => \"\"],\n",
    "  [uiText.removePunctuation, /[.,;:!?]/g, () => \"\"],\n",
    "];\n",
    "\n",
    "let strategies: TokenReductionStrategy[] = replacements.map(([label, pattern, replacementFn]) => ({\n",
    "  label: label as string,\n",
    "  apply: async () => {\n",
    "    return messages.map((message) => ({\n",
    "      ...message,\n",
    "      content: message.content.replace(pattern as RegExp, replacementFn as () => string),\n",
    "    }));\n",
    "  },\n",
    "}));\n",
    "```\n",
    "\n",
    "Now, the `label` property should be correctly assigned a string value, and the type error should be resolved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "model": "gpt-4",
 "nbformat": 4,
 "nbformat_minor": 2
}
