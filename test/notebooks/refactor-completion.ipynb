{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "system"
    ]
   },
   "source": [
    "> You are a code generator that can only answer with python code."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File: 'example.ts'\n",
    "\n",
    "```typescript\n",
    "function greet(name: string) {\n",
    "  console.log(\"Hello, \" + name);\n",
    "}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add the age of the user to the console log.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": [
     "assistant"
    ]
   },
   "outputs": [],
   "source": [
    "with open('example.ts', 'w') as f: f.write(\"\"\"\n",
    "function greet(name: string, age : number) {\n",
    "  console.log(`Hello, ${name}. You are ${age} years old.`);\n",
    "}\n",
    "\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File: '../../src/finishReason.ts'\n",
    "\n",
    "```typescript\n",
    "export enum FinishReason {\n",
    "  length,\n",
    "  contentFilter,\n",
    "  stop,\n",
    "  null,\n",
    "  cancelled\n",
    "}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**My compiler says that FinishReason \"timeout\" is missing. Fix the issue.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "assistant"
    ]
   },
   "outputs": [],
   "source": [
    "with open('../../src/finishReason.ts', 'w') as f: f.write(\"\"\"\n",
    "export enum FinishReason {\n",
    "  length,\n",
    "  contentFilter,\n",
    "  stop,\n",
    "  null,\n",
    "  cancelled,\n",
    "  timeout\n",
    "}\n",
    "\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "File: '../../src/completion.ts'\n",
      "\n",
      " import axios from \"axios\";\n",
      "import {\n",
      "  ChatCompletionRequestMessage,\n",
      "  Configuration,\n",
      "  CreateChatCompletionRequest,\n",
      "  OpenAIApi,\n",
      "} from \"openai\";\n",
      "import {\n",
      "  CancellationToken,\n",
      "  ConfigurationTarget,\n",
      "  NotebookCellKind,\n",
      "  NotebookEdit,\n",
      "  NotebookRange,\n",
      "  QuickPickItem,\n",
      "  WorkspaceEdit,\n",
      "  window,\n",
      "  workspace,\n",
      "} from \"vscode\";\n",
      "import {\n",
      "  appendTextToCell,\n",
      "  convertCellsToMessages,\n",
      "  insertCell,\n",
      "} from \"./cellUtils\";\n",
      "import { CompletionType } from \"./completionType\";\n",
      "import { FinishReason } from \"./finishReason\";\n",
      "import { bufferWholeChunks, streamChatCompletion } from \"./streamUtils\";\n",
      "import { UIProgress } from \"./uiProgress\";\n",
      "import { encoding_for_model } from \"@dqbd/tiktoken\";\n",
      "import { QuickPickItemKind } from \"vscode\";\n",
      "\n",
      "const SENDING_COMPLETION_REQUEST = \"Sending ChatCompletion request\";\n",
      "const RECEIVING_TOKENS = \"Receiving tokens...\";\n",
      "const pythonCodeBlockStart = \"```python\\n\";\n",
      "const codeBlockEnd = \"```\";\n",
      "\n",
      "export const output = window.createOutputChannel(\"Notebook ChatCompletion\");\n",
      "\n",
      "export async function generateCompletion(\n",
      "  cellIndex: number,\n",
      "  completionType: CompletionType,\n",
      "  progress: UIProgress,\n",
      "  token: CancellationToken,\n",
      "  previousFinishReason: FinishReason\n",
      "): Promise<FinishReason> {\n",
      "  const editor = window.activeNotebookEditor!;\n",
      "  let messages = await convertCellsToMessages(cellIndex, completionType);\n",
      "  let currentKind: NotebookCellKind | undefined = undefined;\n",
      "\n",
      "  const openAIApiKey = await getOpenAIApiKey();\n",
      "\n",
      "  if (!openAIApiKey) {\n",
      "    throw new Error(\"OpenAI API key is not set\");\n",
      "  }\n",
      "\n",
      "  const openai = new OpenAIApi(new Configuration({ apiKey: openAIApiKey }));\n",
      "\n",
      "  const tokenSource = axios.CancelToken.source();\n",
      "  token.onCancellationRequested(() => tokenSource.cancel());\n",
      "\n",
      "  const notebookMetadata = editor.notebook.metadata.custom;\n",
      "\n",
      "  \n",
      "  \n",
      "  const defaultModel = workspace\n",
      "    .getConfiguration()\n",
      "    .get<string>(\"notebook-chatcompletion.defaultModel\");\n",
      "\n",
      "  const model = notebookMetadata?.model ?? defaultModel;\n",
      "  const temperature = notebookMetadata?.temperature ?? 0;\n",
      "\n",
      "  let limit: number | null = null;\n",
      "\n",
      "  switch (model) {\n",
      "    case \"gpt-4\":\n",
      "    case \"gpt-4-0314\":\n",
      "      limit = 8192;\n",
      "      break;\n",
      "\n",
      "    case \"gpt-4-32k\":\n",
      "    case \"gpt-4-32k-0314\":\n",
      "      limit = 32768;\n",
      "      break;\n",
      "\n",
      "    case \"gpt-3.5-turbo\":\n",
      "    case \"gpt-3.5-turbo-0301\":\n",
      "      limit = 4096;\n",
      "      break;\n",
      "\n",
      "    default:\n",
      "      break;\n",
      "  }\n",
      "\n",
      "  const msgText = JSON.stringify(messages);\n",
      "  const totalTokenCount = countTokens(msgText, model);\n",
      "\n",
      "  if (limit !== null && totalTokenCount > limit) {\n",
      "    const tokenOverflow = limit - totalTokenCount;\n",
      "    \n",
      "    const msgText = messages.map((x) => x.content).join();\n",
      "    const contentTokenCount = countTokens(msgText, model);\n",
      "\n",
      "    const reducedMessages = await applyTokenReductionStrategies(\n",
      "      messages,\n",
      "      tokenOverflow,\n",
      "      contentTokenCount,\n",
      "      limit,\n",
      "      model\n",
      "    );\n",
      "\n",
      "    if (!reducedMessages) {\n",
      "      return FinishReason.cancelled;\n",
      "    }\n",
      "\n",
      "    messages = reducedMessages;\n",
      "  }\n",
      "\n",
      "  let requestParams: CreateChatCompletionRequest = {\n",
      "    model: model,\n",
      "    messages,\n",
      "    stream: true,\n",
      "    temperature: temperature,\n",
      "  };\n",
      "\n",
      "  if (limit) {\n",
      "    \n",
      "    const reducedMsgText = JSON.stringify(messages);\n",
      "    const reducedTokenCount = countTokens(reducedMsgText, model);\n",
      "    requestParams.max_tokens = limit - reducedTokenCount;\n",
      "\n",
      "    if (requestParams.max_tokens < 1) {\n",
      "      const result = await window.showInformationMessage(\n",
      "        `The request is estimated to be ${-requestParams.max_tokens} tokens over the limit (including tokens consumed by the request format) and will likely be rejected from the OpenAI API. Do you still want to proceed?`,\n",
      "        { modal: true },\n",
      "        \"Yes\"\n",
      "      );\n",
      "      if (result !== \"Yes\") {\n",
      "        return FinishReason.cancelled;\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "\n",
      "  requestParams = addParametersFromMetadata(notebookMetadata, requestParams);\n",
      "\n",
      "  output.appendLine(\"\\n\" + JSON.stringify(requestParams, undefined, 2) + \"\\n\");\n",
      "  progress.report({ increment: 1, message: SENDING_COMPLETION_REQUEST });\n",
      "\n",
      "  const response = await openai.createChatCompletion(requestParams, {\n",
      "    cancelToken: tokenSource.token,\n",
      "    responseType: \"stream\",\n",
      "  });\n",
      "\n",
      "  for await (let textToken of bufferWholeChunks(\n",
      "    streamChatCompletion(response, token)\n",
      "  )) {\n",
      "    \n",
      "    if (Object.values(FinishReason).includes(textToken as FinishReason)) {\n",
      "      switch (textToken) {\n",
      "        case FinishReason.length:\n",
      "          output.append(\"FINISH_REASON_LENGTH\" + \"\\n\");\n",
      "          break;\n",
      "        case FinishReason.contentFilter:\n",
      "          output.append(\"FINISH_REASON_CONTENTFILTER\" + \"\\n\");\n",
      "          break;\n",
      "        case FinishReason.stop:\n",
      "          output.append(\"FINISH_REASON_STOP\" + \"\\n\");\n",
      "          break;\n",
      "      }\n",
      "\n",
      "      const currentCell = editor.notebook.cellAt(cellIndex);\n",
      "      const text = currentCell.document.getText();\n",
      "\n",
      "      \n",
      "      \n",
      "      if (!/\\S/.test(text)) {\n",
      "        const edit = new WorkspaceEdit();\n",
      "        edit.set(currentCell.notebook.uri, [\n",
      "          NotebookEdit.deleteCells(\n",
      "            new NotebookRange(currentCell.index, currentCell.index + 1)\n",
      "          ),\n",
      "        ]);\n",
      "        await workspace.applyEdit(edit);\n",
      "      }\n",
      "\n",
      "      return textToken as FinishReason;\n",
      "    } else {\n",
      "      \n",
      "      output.append(textToken.toString());\n",
      "    }\n",
      "\n",
      "    if (typeof textToken !== \"string\") {\n",
      "      throw new Error(\"Invalid state: unknown stream result: \" + textToken);\n",
      "    }\n",
      "\n",
      "    if (textToken.includes(pythonCodeBlockStart)) {\n",
      "      \n",
      "      \n",
      "      currentKind = NotebookCellKind.Code;\n",
      "\n",
      "      cellIndex = await insertCell(editor, cellIndex, currentKind, \"python\");\n",
      "      textToken = textToken.replace(pythonCodeBlockStart, \"\");\n",
      "    } else if (\n",
      "      textToken.includes(codeBlockEnd) &&\n",
      "      currentKind === NotebookCellKind.Code\n",
      "    ) {\n",
      "      textToken = textToken.replace(codeBlockEnd, \"\");\n",
      "\n",
      "      \n",
      "      \n",
      "      currentKind = NotebookCellKind.Markup;\n",
      "      cellIndex = await insertCell(editor, cellIndex, currentKind, \"markdown\");\n",
      "    }\n",
      "\n",
      "    if (currentKind === undefined) {\n",
      "      \n",
      "      cellIndex = await insertCell(\n",
      "        editor,\n",
      "        cellIndex,\n",
      "        NotebookCellKind.Markup,\n",
      "        \"markdown\"\n",
      "      );\n",
      "      currentKind = NotebookCellKind.Markup;\n",
      "    }\n",
      "\n",
      "    \n",
      "    await appendTextToCell(editor, cellIndex, textToken);\n",
      "\n",
      "    progress.report({ increment: 0.5, message: RECEIVING_TOKENS });\n",
      "  }\n",
      "\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  return FinishReason.length;\n",
      "}\n",
      "\n",
      "function addParametersFromMetadata(\n",
      "  notebookMetadata: any,\n",
      "  requestParams: CreateChatCompletionRequest\n",
      ") {\n",
      "  const editor = window.activeNotebookEditor;\n",
      "  if (editor && notebookMetadata) {\n",
      "    if (editor.notebook.metadata.custom?.top_p) {\n",
      "      requestParams = {\n",
      "        ...requestParams,\n",
      "        \n",
      "        top_p: editor.notebook.metadata.custom.top_p,\n",
      "      };\n",
      "    }\n",
      "    if (editor.notebook.metadata.custom?.n) {\n",
      "      requestParams = {\n",
      "        ...requestParams,\n",
      "        n: editor.notebook.metadata.custom.n,\n",
      "      };\n",
      "    }\n",
      "    if (editor.notebook.metadata.custom?.max_tokens) {\n",
      "      requestParams.max_tokens = editor.notebook.metadata.custom.max_tokens;\n",
      "    }\n",
      "    if (editor.notebook.metadata.custom?.presence_penalty) {\n",
      "      requestParams = {\n",
      "        ...requestParams,\n",
      "        \n",
      "        presence_penalty: editor.notebook.metadata.custom.presence_penalty,\n",
      "      };\n",
      "    }\n",
      "    if (editor.notebook.metadata.custom?.frequency_penalty) {\n",
      "      requestParams = {\n",
      "        ...requestParams,\n",
      "        \n",
      "        frequency_penalty: editor.notebook.metadata.custom.frequency_penalty,\n",
      "      };\n",
      "    }\n",
      "    if (editor.notebook.metadata.custom?.logit_bias) {\n",
      "      requestParams = {\n",
      "        ...requestParams,\n",
      "        \n",
      "        logit_bias: editor.notebook.metadata.custom.logit_bias,\n",
      "      };\n",
      "    }\n",
      "    if (editor.notebook.metadata.custom?.user) {\n",
      "      requestParams = {\n",
      "        ...requestParams,\n",
      "        user: editor.notebook.metadata.custom.top_p,\n",
      "      };\n",
      "    }\n",
      "  }\n",
      "  return requestParams;\n",
      "}\n",
      "\n",
      "async function getOpenAIApiKey(): Promise<string> {\n",
      "  let openaiApiKey = workspace\n",
      "    .getConfiguration()\n",
      "    .get<string>(\"notebook-chatcompletion.openaiApiKey\");\n",
      "  if (!openaiApiKey) {\n",
      "    \n",
      "    openaiApiKey = await window.showInputBox({\n",
      "      prompt: \"Enter your OpenAI API Key:\",\n",
      "      validateInput: (value) =>\n",
      "        value.trim().length > 0 ? null : \"API Key cannot be empty\",\n",
      "    });\n",
      "\n",
      "    \n",
      "    if (openaiApiKey) {\n",
      "      await workspace\n",
      "        .getConfiguration()\n",
      "        .update(\n",
      "          \"notebook-chatcompletion.openaiApiKey\",\n",
      "          openaiApiKey,\n",
      "          ConfigurationTarget.Global\n",
      "        );\n",
      "\n",
      "      await window.showInformationMessage(\n",
      "        \"Please note that the model is set to GPT-4 by default, which you may not be able to access yet. As a result, the API may return an HTTP 404 error. You can change the 'Default Model' setting to another model or use the 'Set Model' command in the menu to set the model for a specific notebook.\",\n",
      "        { modal: true }\n",
      "      );\n",
      "    } else {\n",
      "      \n",
      "      window.showErrorMessage(\n",
      "        \"OpenAI API Key is required for Notebook ChatCompletion to work.\",\n",
      "        { modal: true }\n",
      "      );\n",
      "      return \"\";\n",
      "    }\n",
      "  }\n",
      "\n",
      "  return openaiApiKey;\n",
      "}\n",
      "\n",
      "type TokenReductionStrategy = QuickPickItem & {\n",
      "  apply: Function;\n",
      "  savedTokens?: number;\n",
      "};\n",
      "\n",
      "async function applyTokenReductionStrategies(\n",
      "  messages: ChatCompletionRequestMessage[],\n",
      "  tokenOverflowCount: number,\n",
      "  totalTokenCount: number,\n",
      "  limit: number,\n",
      "  model: string\n",
      "): Promise<ChatCompletionRequestMessage[] | null> {\n",
      "  let strategies: TokenReductionStrategy[] = [\n",
      "    {\n",
      "      label: \"Remove all Cell Output\",\n",
      "      apply: async () => {\n",
      "        return messages.filter(\n",
      "          (message) => !message.content.startsWith(\"Output from previous code:\")\n",
      "        );\n",
      "      },\n",
      "    },\n",
      "    {\n",
      "      label: \"Remove all VSCode Problems\",\n",
      "      apply: async () => {\n",
      "        return messages.filter(\n",
      "          (message) =>\n",
      "            !message.content.startsWith(\n",
      "              \"Problems reported by VSCode from previous code:\"\n",
      "            )\n",
      "        );\n",
      "      },\n",
      "    },\n",
      "    {\n",
      "      label: \"Remove Spaces\",\n",
      "      apply: async () => {\n",
      "        return messages.map((message) => ({\n",
      "          ...message,\n",
      "          content: message.content.replace(/ /g, \"\"),\n",
      "        }));\n",
      "      },\n",
      "    },\n",
      "    {\n",
      "      label: \"Remove Line-breaks\",\n",
      "      apply: async () => {\n",
      "        return messages.map((message) => ({\n",
      "          ...message,\n",
      "          content: message.content.replace(/\\n/g, \"\"),\n",
      "        }));\n",
      "      },\n",
      "    },\n",
      "    {\n",
      "      label: \"Remove Punctuations\",\n",
      "      apply: async () => {\n",
      "        return messages.map((message) => ({\n",
      "          ...message,\n",
      "          content: message.content.replace(/[.,;:!?]/g, \"\"),\n",
      "        }));\n",
      "      },\n",
      "    },\n",
      "  ];\n",
      "\n",
      "  \n",
      "  for (const strategy of strategies) {\n",
      "    const reducedMessages = await strategy.apply();\n",
      "    const reducedTokenCount = countTotalTokens(reducedMessages, model);\n",
      "    const savedTokens = totalTokenCount - reducedTokenCount;\n",
      "    strategy.savedTokens = savedTokens;\n",
      "    strategy.description = `${savedTokens} tokens`;\n",
      "  }\n",
      "\n",
      "  \n",
      "  strategies = strategies.filter((s) =>\n",
      "    s.savedTokens ? s.savedTokens > 1 : false\n",
      "  );\n",
      "\n",
      "  const maxPossibleSaving = strategies\n",
      "    .map((x) => x.savedTokens ?? 0)\n",
      "    .reduce((prev, current) => prev + current);\n",
      "\n",
      "  if (maxPossibleSaving < tokenOverflowCount) {\n",
      "    window.showInformationMessage(\n",
      "      `If we applied every token reduction strategy available, you would still be ${\n",
      "        tokenOverflowCount - maxPossibleSaving\n",
      "      } over the limit of the '${model}' model. Please reduce the size of the content.`,\n",
      "      { modal: true }\n",
      "    );\n",
      "  }\n",
      "\n",
      "  \n",
      "  const selectedStrategies = await window.showQuickPick(strategies, {\n",
      "    canPickMany: true,\n",
      "    title: \"Too many tokens\",\n",
      "    placeHolder: \"Select one or more strategies to reduce the token count\",\n",
      "  });\n",
      "\n",
      "  if (!selectedStrategies) {\n",
      "    return null;\n",
      "  }\n",
      "\n",
      "  \n",
      "  let reducedMessages = messages;\n",
      "  for (const strategy of selectedStrategies) {\n",
      "    reducedMessages = await strategy.apply(reducedMessages);\n",
      "  }\n",
      "\n",
      "  \n",
      "  const reducedTokenCount = countTotalTokens(reducedMessages, model);\n",
      "  if (reducedTokenCount > limit) {\n",
      "    window.showErrorMessage(\n",
      "      \"The selected strategies do not reduce tokens below the limit.\"\n",
      "    );\n",
      "    return null;\n",
      "  }\n",
      "\n",
      "  return reducedMessages;\n",
      "}\n",
      "\n",
      "function countTokens(text: string, model: any): number {\n",
      "  const enc = encoding_for_model(model);\n",
      "  const tokenCount = enc.encode(text).length;\n",
      "  enc.free();\n",
      "  return tokenCount;\n",
      "}\n",
      "\n",
      "function countTotalTokens(\n",
      "  messages: ChatCompletionRequestMessage[],\n",
      "  model: string\n",
      "): number {\n",
      "  return messages.reduce((accumulator, message) => {\n",
      "    return accumulator + countTokens(message.content, model);\n",
      "  }, 0);\n",
      "}\n",
      " \n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import List\n",
    "import re\n",
    "\n",
    "def print_file_content(file_path: Path):\n",
    "    content = file_path.read_text()\n",
    "\n",
    "    # Remove single-line comments\n",
    "    content = re.sub(r\"//.*\", \"\", content)\n",
    "\n",
    "    # Remove multi-line comments\n",
    "    content = re.sub(r\"/\\*[\\s\\S]*?\\*/\", \"\", content)\n",
    "\n",
    "    print(f\"\\n\\nFile: '{file_path}'\")\n",
    "    print(\"\\n\", content, \"\")\n",
    "\n",
    "def process_files(file_patterns: List[str]):\n",
    "    for pattern in file_patterns:\n",
    "        for file in Path().glob(pattern):\n",
    "            print_file_content(file)\n",
    "\n",
    "file_patterns = [\"../../src/completion.ts\"]\n",
    "process_files(file_patterns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- **Let's first perform an exhaustive code review on the previous completion.ts file and then devise a plan to refactor the code.**\n",
    "- **Apply all of your proposed refactorings the same way you did in Example 1 and 2**\n",
    "- **The completion.ts code will be processed by a large language model with limited window size (max tokens). Therefore, your code must attempt to reduce the amount of text tokens of your generated code by applying techniques including but not limited to code inlining, shortening variable names, using python built-in functions when possible, and any other idea for text token reduction you can come up with.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "assistant"
    ]
   },
   "source": [
    "After reviewing the `completion.ts` file, here are some proposed refactorings:\n",
    "\n",
    "1. Shorten variable and function names.\n",
    "2. Inline some functions that are only used once.\n",
    "3. Remove unnecessary comments.\n",
    "4. Combine multiple variable assignments into a single line when possible.\n",
    "5. Use Python built-in functions when possible.\n",
    "\n",
    "Now, let's apply these refactorings to the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "assistant"
    ]
   },
   "outputs": [],
   "source": [
    "with open('../../src/completion.ts', 'w') as f: f.write(\"\"\"\n",
    "import axios from \"axios\";\n",
    "import {\n",
    "  ChatCompletionRequestMessage,\n",
    "  Configuration,\n",
    "  CreateChatCompletionRequest,\n",
    "  OpenAIApi,\n",
    "} from \"openai\";\n",
    "import {\n",
    "  CancellationToken,\n",
    "  ConfigurationTarget,\n",
    "  NotebookCellKind,\n",
    "  NotebookEdit,\n",
    "  NotebookRange,\n",
    "  QuickPickItem,\n",
    "  WorkspaceEdit,\n",
    "  window,\n",
    "  workspace,\n",
    "} from \"vscode\";\n",
    "import {\n",
    "  appendTextToCell,\n",
    "  convertCellsToMessages,\n",
    "  insertCell,\n",
    "} from \"./cellUtils\";\n",
    "import { CompletionType } from \"./completionType\";\n",
    "import { FinishReason } from \"./finishReason\";\n",
    "import { bufferWholeChunks, streamChatCompletion } from \"./streamUtils\";\n",
    "import { UIProgress } from \"./uiProgress\";\n",
    "import { encoding_for_model } from \"@dqbd/tiktoken\";\n",
    "\n",
    "const output = window.createOutputChannel(\"Notebook ChatCompletion\");\n",
    "\n",
    "export async function generateCompletion(\n",
    "  ci: number,\n",
    "  ct: CompletionType,\n",
    "  p: UIProgress,\n",
    "  t: CancellationToken,\n",
    "  prevFR: FinishReason\n",
    "): Promise<FinishReason> {\n",
    "  const e = window.activeNotebookEditor!;\n",
    "  let msgs = await convertCellsToMessages(ci, ct);\n",
    "  let ck: NotebookCellKind | undefined = undefined;\n",
    "\n",
    "  const openaiApiKey = await getOpenAIApiKey();\n",
    "\n",
    "  if (!openaiApiKey) {\n",
    "    throw new Error(\"OpenAI API key is not set\");\n",
    "  }\n",
    "\n",
    "  const openai = new OpenAIApi(new Configuration({ apiKey: openaiApiKey }));\n",
    "\n",
    "  const tokenSource = axios.CancelToken.source();\n",
    "  t.onCancellationRequested(() => tokenSource.cancel());\n",
    "\n",
    "  const nbMetadata = e.notebook.metadata.custom;\n",
    "\n",
    "  const defaultModel = workspace\n",
    "    .getConfiguration()\n",
    "    .get<string>(\"notebook-chatcompletion.defaultModel\");\n",
    "\n",
    "  const model = nbMetadata?.model ?? defaultModel;\n",
    "  const temperature = nbMetadata?.temperature ?? 0;\n",
    "\n",
    "  let limit: number | null = null;\n",
    "\n",
    "  switch (model) {\n",
    "    case \"gpt-4\":\n",
    "    case \"gpt-4-0314\":\n",
    "      limit = 8192;\n",
    "      break;\n",
    "\n",
    "    case \"gpt-4-32k\":\n",
    "    case \"gpt-4-32k-0314\":\n",
    "      limit = 32768;\n",
    "      break;\n",
    "\n",
    "    case \"gpt-3.5-turbo\":\n",
    "    case \"gpt-3.5-turbo-0301\":\n",
    "      limit = 4096;\n",
    "      break;\n",
    "\n",
    "    default:\n",
    "      break;\n",
    "  }\n",
    "\n",
    "  const msgText = JSON.stringify(msgs);\n",
    "  const totalTokenCount = countTokens(msgText, model);\n",
    "\n",
    "  if (limit !== null && totalTokenCount > limit) {\n",
    "    const tokenOverflow = limit - totalTokenCount;\n",
    "    \n",
    "    const msgText = msgs.map((x) => x.content).join();\n",
    "    const contentTokenCount = countTokens(msgText, model);\n",
    "\n",
    "    const reducedMessages = await applyTokenReductionStrategies(\n",
    "      msgs,\n",
    "      tokenOverflow,\n",
    "      contentTokenCount,\n",
    "      limit,\n",
    "      model\n",
    "    );\n",
    "\n",
    "    if (!reducedMessages) {\n",
    "      return FinishReason.cancelled;\n",
    "    }\n",
    "\n",
    "    msgs = reducedMessages;\n",
    "  }\n",
    "\n",
    "  let reqParams: CreateChatCompletionRequest = {\n",
    "    model: model,\n",
    "    messages: msgs,\n",
    "    stream: true,\n",
    "    temperature: temperature,\n",
    "  };\n",
    "\n",
    "  if (limit) {\n",
    "    \n",
    "    const reducedMsgText = JSON.stringify(msgs);\n",
    "    const reducedTokenCount = countTokens(reducedMsgText, model);\n",
    "    reqParams.max_tokens = limit - reducedTokenCount;\n",
    "\n",
    "    if (reqParams.max_tokens < 1) {\n",
    "      const result = await window.showInformationMessage(\n",
    "        `The request is estimated to be ${-reqParams.max_tokens} tokens over the limit (including tokens consumed by the request format) and will likely be rejected from the OpenAI API. Do you still want to proceed?`,\n",
    "        { modal: true },\n",
    "        \"Yes\"\n",
    "      );\n",
    "      if (result !== \"Yes\") {\n",
    "        return FinishReason.cancelled;\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "\n",
    "  reqParams = addParametersFromMetadata(nbMetadata, reqParams);\n",
    "\n",
    "  output.appendLine(\"\\n\" + JSON.stringify(reqParams, undefined, 2) + \"\\n\");\n",
    "  p.report({ increment: 1, message: \"Sending ChatCompletion request\" });\n",
    "\n",
    "  const response = await openai.createChatCompletion(reqParams, {\n",
    "    cancelToken: tokenSource.token,\n",
    "    responseType: \"stream\",\n",
    "  });\n",
    "\n",
    "  for await (let textToken of bufferWholeChunks(\n",
    "    streamChatCompletion(response, t)\n",
    "  )) {\n",
    "    \n",
    "    if (Object.values(FinishReason).includes(textToken as FinishReason)) {\n",
    "      switch (textToken) {\n",
    "        case FinishReason.length:\n",
    "          output.append(\"FINISH_REASON_LENGTH\" + \"\\n\");\n",
    "          break;\n",
    "        case FinishReason.contentFilter:\n",
    "          output.append(\"FINISH_REASON_CONTENTFILTER\" + \"\\n\");\n",
    "          break;\n",
    "        case FinishReason.stop:\n",
    "          output.append(\"FINISH_REASON_STOP\" + \"\\n\");\n",
    "          break;\n",
    "      }\n",
    "\n",
    "      const currentCell = e.notebook.cellAt(ci);\n",
    "      const text = currentCell.document.getText();\n",
    "\n",
    "      \n",
    "      \n",
    "      if (!/\\S/.test(text)) {\n",
    "        const edit = new WorkspaceEdit();\n",
    "        edit.set(currentCell.notebook.uri, [\n",
    "          NotebookEdit.deleteCells(\n",
    "            new NotebookRange(currentCell.index, currentCell.index + 1)\n",
    "          ),\n",
    "        ]);\n",
    "        await workspace.applyEdit(edit);\n",
    "      }\n",
    "\n",
    "      return textToken as FinishReason;\n",
    "    } else {\n",
    "      \n",
    "      output.append(textToken.toString());\n",
    "    }\n",
    "\n",
    "    if (typeof textToken !== \"string\") {\n",
    "      throw new Error(\"Invalid state: unknown stream result: \" + textToken);\n",
    "    }\n",
    "\n",
    "    if (textToken.includes(\"```python\\\\n\")) {\n",
    "      \n",
    "      \n",
    "      ck = NotebookCellKind.Code;\n",
    "\n",
    "      ci = await insertCell(e, ci, ck, \"python\");\n",
    "      textToken = textToken.replace(\"```python\\\\n\", \"\");\n",
    "    } else if (\n",
    "      textToken.includes(\"```\") &&\n",
    "      ck === NotebookCellKind.Code\n",
    "    ) {\n",
    "      textToken = textToken.replace(\"```\", \"\");\n",
    "\n",
    "      \n",
    "      \n",
    "      ck = NotebookCellKind.Markup;\n",
    "      ci = await insertCell(e, ci, ck, \"markdown\");\n",
    "    }\n",
    "\n",
    "    if (ck === undefined) {\n",
    "      \n",
    "      ci = await insertCell(\n",
    "        e,\n",
    "        ci,\n",
    "        NotebookCellKind.Markup,\n",
    "        \"markdown\"\n",
    "      );\n",
    "      ck = NotebookCellKind.Markup;\n",
    "    }\n",
    "\n",
    "    \n",
    "    await appendTextToCell(e, ci, textToken);\n",
    "\n",
    "    p.report({ increment: 0.5, message: \"Receiving tokens...\" });\n",
    "  }\n",
    "\n",
    "  return FinishReason.length;\n",
    "}\n",
    "\n",
    "function addParametersFromMetadata(\n",
    "  nbMetadata: any,\n",
    "  reqParams: CreateChatCompletionRequest\n",
    ") {\n",
    "  const e = window.activeNotebookEditor;\n",
    "  if (e && nbMetadata) {\n",
    "    if (e.notebook.metadata.custom?.top_p) {\n",
    "      reqParams = {\n",
    "        ...reqParams,\n",
    "        top_p: e.notebook.metadata.custom.top_p,\n",
    "      };\n",
    "    }\n",
    "    if (e.notebook.metadata.custom?.n) {\n",
    "      reqParams = {\n",
    "        ...reqParams,\n",
    "        n: e.notebook.metadata.custom.n,\n",
    "      };\n",
    "    }\n",
    "    if (e.notebook.metadata.custom?.max_tokens) {\n",
    "      reqParams.max_tokens = e.notebook.metadata.custom.max_tokens;\n",
    "    }\n",
    "    if (e.notebook.metadata.custom?.presence_penalty) {\n",
    "      reqParams = {\n",
    "        ...reqParams,\n",
    "        presence_penalty: e.notebook.metadata.custom.presence_penalty,\n",
    "      };\n",
    "    }\n",
    "    if (e.notebook.metadata.custom?.frequency_penalty) {\n",
    "      reqParams = {\n",
    "        ...reqParams,\n",
    "        frequency_penalty: e.notebook.metadata.custom.frequency_penalty,\n",
    "      };\n",
    "    }\n",
    "    if (e.notebook.metadata.custom?.logit_bias) {\n",
    "      reqParams = {\n",
    "        ...reqParams,\n",
    "        logit_bias: e.notebook.metadata.custom.logit_bias,\n",
    "      };\n",
    "    }\n",
    "    if (e.notebook.metadata.custom?.user) {\n",
    "      reqParams = {\n",
    "        ...reqParams,\n",
    "        user: e.notebook.metadata.custom.top_p,\n",
    "      };\n",
    "    }\n",
    "  }\n",
    "  return reqParams;\n",
    "}\n",
    "\n",
    "async function getOpenAIApiKey(): Promise<string> {\n",
    "  let apiKey = workspace\n",
    "    .getConfiguration()\n",
    "    .get<string>(\"notebook-chatcompletion.openaiApiKey\");\n",
    "  if (!apiKey) {\n",
    "    \n",
    "    apiKey = await window.showInputBox({\n",
    "      prompt: \"Enter your OpenAI API Key:\",\n",
    "      validateInput: (value) =>\n",
    "        value.trim().length > 0 ? null : \"API Key cannot be empty\",\n",
    "    });\n",
    "\n",
    "    \n",
    "    if (apiKey) {\n",
    "      await workspace\n",
    "        .getConfiguration()\n",
    "        .update(\n",
    "          \"notebook-chatcompletion.openaiApiKey\",\n",
    "          apiKey,\n",
    "          ConfigurationTarget.Global\n",
    "        );\n",
    "\n",
    "      await window.showInformationMessage(\n",
    "        \"Please note that the model is set to GPT-4 by default, which you may not be able to access yet. As a result, the API may return an HTTP 404 error. You can change the 'Default Model' setting to another model or use the 'Set Model' command in the menu to set the model for a specific notebook.\",\n",
    "        { modal: true }\n",
    "      );\n",
    "    } else {\n",
    "      \n",
    "      window.showErrorMessage(\n",
    "        \"OpenAI API Key is required for Notebook ChatCompletion to work.\",\n",
    "        { modal: true }\n",
    " );\n",
    "      return \"\";\n",
    "    }\n",
    "  }\n",
    "\n",
    "  return apiKey;\n",
    "}\n",
    "\n",
    "type TokenReductionStrategy = QuickPickItem & {\n",
    "  apply: Function;\n",
    "  savedTokens?: number;\n",
    "};\n",
    "\n",
    "async function applyTokenReductionStrategies(\n",
    "  msgs: ChatCompletionRequestMessage[],\n",
    "  tokenOverflowCount: number,\n",
    "  totalTokenCount: number,\n",
    "  limit: number,\n",
    "  model: string\n",
    "): Promise<ChatCompletionRequestMessage[] | null> {\n",
    "  let strategies: TokenReductionStrategy[] = [\n",
    "    {\n",
    "      label: \"Remove all Cell Output\",\n",
    "      apply: async () => {\n",
    "        return msgs.filter(\n",
    "          (message) => !message.content.startsWith(\"Output from previous code:\")\n",
    "        );\n",
    "      },\n",
    "    },\n",
    "    {\n",
    "      label: \"Remove all VSCode Problems\",\n",
    "      apply: async () => {\n",
    "        return msgs.filter(\n",
    "          (message) =>\n",
    "            !message.content.startsWith(\n",
    "              \"Problems reported by VSCode from previous code:\"\n",
    "            )\n",
    "        );\n",
    "      },\n",
    "    },\n",
    "    {\n",
    "      label: \"Remove Spaces\",\n",
    "      apply: async () => {\n",
    "        return msgs.map((message) => ({\n",
    "          ...message,\n",
    "          content: message.content.replace(/ /g, \"\"),\n",
    "        }));\n",
    "      },\n",
    "    },\n",
    "    {\n",
    "      label: \"Remove Line-breaks\",\n",
    "      apply: async () => {\n",
    "        return msgs.map((message) => ({\n",
    "          ...message,\n",
    "          content: message.content.replace(/\\n/g, \"\"),\n",
    "        }));\n",
    "      },\n",
    "    },\n",
    "    {\n",
    "      label: \"Remove Punctuations\",\n",
    "      apply: async () => {\n",
    "        return msgs.map((message) => ({\n",
    "          ...message,\n",
    "          content: message.content.replace(/[.,;:!?]/g, \"\"),\n",
    "        }));\n",
    "      },\n",
    "    },\n",
    "  ];\n",
    "  \n",
    "  for (const strategy of strategies) {\n",
    "    const reducedMessages = await strategy.apply();\n",
    "    const reducedTokenCount = countTotalTokens(reducedMessages, model);\n",
    "    const savedTokens = totalTokenCount - reducedTokenCount;\n",
    "    strategy.savedTokens = savedTokens;\n",
    "    strategy.description = `${savedTokens} tokens`;\n",
    "  }\n",
    "  \n",
    "  strategies = strategies.filter((s) =>\n",
    "    s.savedTokens ? s.savedTokens > 1 : false\n",
    "  );\n",
    "\n",
    "  const maxPossibleSaving = strategies\n",
    "    .map((x) => x.savedTokens ?? 0)\n",
    "    .reduce((prev, current) => prev + current);\n",
    "\n",
    "  if (maxPossibleSaving < tokenOverflowCount) {\n",
    "    window.showInformationMessage(\n",
    "      `If we applied every token reduction strategy available, you would still be ${\n",
    "        tokenOverflowCount - maxPossibleSaving\n",
    "      } over the limit of the '${model}' model. Please reduce the size of the content.`,\n",
    "      { modal: true }\n",
    "    );\n",
    "  }\n",
    "  \n",
    "  const selectedStrategies = await window.showQuickPick(strategies, {\n",
    "    canPickMany: true,\n",
    "    title: \"Too many tokens\",\n",
    "    placeHolder: \"Select one or more strategies to reduce the token count\",\n",
    "  });\n",
    "\n",
    "  if (!selectedStrategies) {\n",
    "    return null;\n",
    "  }\n",
    "  \n",
    "  let reducedMessages = msgs;\n",
    "  for (const strategy of selectedStrategies) {\n",
    "    reducedMessages = await strategy.apply(reducedMessages);\n",
    "  }\n",
    "  \n",
    "  const reducedTokenCount = countTotalTokens(reducedMessages, model);\n",
    "  if (reducedTokenCount > limit) {\n",
    "    window.showErrorMessage(\n",
    "      \"The selected strategies do not reduce tokens below the limit.\"\n",
    "    );\n",
    "    return null;\n",
    "  }\n",
    "\n",
    "  return reducedMessages;\n",
    "}\n",
    "\n",
    "function countTokens(text: string, model: any): number {\n",
    "  const enc = encoding_for_model(model);\n",
    "  const tokenCount = enc.encode(text).length;\n",
    "  enc.free();\n",
    "  return tokenCount;\n",
    "}\n",
    "\n",
    "function countTotalTokens(\n",
    "  msgs: ChatCompletionRequestMessage[],\n",
    "  model: string\n",
    "): number {\n",
    "  return msgs.reduce((accumulator, message) => {\n",
    "    return accumulator + countTokens(message.content, model);\n",
    "  }, 0);\n",
    "}\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "model": "gpt-4",
 "nbformat": 4,
 "nbformat_minor": 2
}
