{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "system"
    ]
   },
   "source": [
    "> You are a code generator and reviewer that can only answer with python code."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File: 'example.ts'\n",
    "\n",
    "```typescript\n",
    "function greet(name: string) {\n",
    "  console.log(\"Hello, \" + name);\n",
    "}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add the age of the user to the console log.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": [
     "assistant"
    ]
   },
   "outputs": [],
   "source": [
    "with open('example.ts', 'w') as f: f.write(\"\"\"\n",
    "function greet(name: string, age : number) {\n",
    "  console.log(`Hello, ${name}. You are ${age} years old.`);\n",
    "}\n",
    "\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File: '../../src/finishReason.ts'\n",
    "\n",
    "```typescript\n",
    "export enum FinishReason {\n",
    "  length,\n",
    "  contentFilter,\n",
    "  stop,\n",
    "  null,\n",
    "  cancelled\n",
    "}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**My compiler says that FinishReason \"timeout\" is missing. Fix the issue.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "assistant"
    ]
   },
   "outputs": [],
   "source": [
    "with open('../../src/finishReason.ts', 'w') as f: f.write(\"\"\"\n",
    "export enum FinishReason {\n",
    "  length,\n",
    "  contentFilter,\n",
    "  stop,\n",
    "  null,\n",
    "  cancelled,\n",
    "  timeout\n",
    "}\n",
    "\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "File: '../../src/completion.ts'\n",
      "\n",
      " import axios, { AxiosResponse } from \"axios\";\n",
      "import { Configuration, CreateChatCompletionRequest, CreateChatCompletionResponse, OpenAIApi } from \"openai\";\n",
      "import { CancellationToken, NotebookCellKind, NotebookEdit, NotebookRange, WorkspaceEdit, window, workspace } from \"vscode\";\n",
      "import { appendTextToCell, convertCellsToMessages, insertCell } from \"./cellUtils\";\n",
      "import { CompletionType } from \"./completionType\";\n",
      "import { addParametersFromMetadata as addNotebookConfigParams, getOpenAIApiKey, getTokenLimit } from \"./config\";\n",
      "import { msgs } from \"./constants\";\n",
      "import { FinishReason } from \"./finishReason\";\n",
      "import { bufferWholeChunks, streamChatCompletion } from \"./streamUtils\";\n",
      "import { applyTokenReductions, countTokens, countTotalTokens } from \"./tokenUtils\";\n",
      "import { UIProgress, waitForUIDispatch } from \"./uiProgress\";\n",
      "import { TiktokenModel } from \"@dqbd/tiktoken\";\n",
      "\n",
      "const output = window.createOutputChannel(\"Notebook ChatCompletion\");\n",
      "\n",
      "async function streamResponse(\n",
      "  response: AxiosResponse<CreateChatCompletionResponse, any>,\n",
      "  cancelToken: CancellationToken,\n",
      "  cellIndex: number,\n",
      "  ck: NotebookCellKind | undefined,\n",
      "  progress: UIProgress\n",
      ") {\n",
      "  const editor = window.activeNotebookEditor!;\n",
      "\n",
      "  for await (let textToken of bufferWholeChunks(streamChatCompletion(response, cancelToken))) {\n",
      "    if (Object.values(FinishReason).includes(textToken as FinishReason)) {\n",
      "      const currentCell = window.activeNotebookEditor!.notebook.cellAt(cellIndex);\n",
      "      const text = currentCell.document.getText();\n",
      "\n",
      "      if (!/\\S/.test(text)) {\n",
      "        const edit = new WorkspaceEdit();\n",
      "        edit.set(currentCell.notebook.uri, [NotebookEdit.deleteCells(new NotebookRange(currentCell.index, currentCell.index + 1))]);\n",
      "        await workspace.applyEdit(edit);\n",
      "      }\n",
      "\n",
      "      return textToken as FinishReason;\n",
      "    } else {\n",
      "      output.append(textToken.toString());\n",
      "    }\n",
      "\n",
      "    if (typeof textToken !== \"string\") {\n",
      "      throw new Error(`Unknown stream result: ${textToken}`);\n",
      "    }\n",
      "\n",
      "    if (textToken.includes(\"```python\\n\")) {\n",
      "      ck = NotebookCellKind.Code;\n",
      "\n",
      "      cellIndex = await insertCell(editor, cellIndex, ck, \"python\");\n",
      "      textToken = textToken.replace(\"```python\\n\", \"\");\n",
      "    } else if (textToken.includes(\"```\") && ck === NotebookCellKind.Code) {\n",
      "      textToken = textToken.replace(\"```\", \"\");\n",
      "\n",
      "      ck = NotebookCellKind.Markup;\n",
      "      cellIndex = await insertCell(editor, cellIndex, ck);\n",
      "    }\n",
      "\n",
      "    if (ck === undefined) {\n",
      "      cellIndex = await insertCell(editor, cellIndex, NotebookCellKind.Markup);\n",
      "      ck = NotebookCellKind.Markup;\n",
      "    }\n",
      "\n",
      "    await appendTextToCell(editor, cellIndex, textToken);\n",
      "\n",
      "    progress.report({ increment: 0.5, message: msgs.receivingTokens });\n",
      "  }\n",
      "\n",
      "  return FinishReason.length;\n",
      "}\n",
      "\n",
      "export async function generateCompletion(\n",
      "  cellIndex: number,\n",
      "  completionType: CompletionType,\n",
      "  progress: UIProgress,\n",
      "  cancelToken: CancellationToken\n",
      "): Promise<FinishReason> {\n",
      "  const e = window.activeNotebookEditor!;\n",
      "  let messages = await convertCellsToMessages(cellIndex, completionType);\n",
      "  let ck: NotebookCellKind | undefined = undefined;\n",
      "\n",
      "  const openaiApiKey = await getOpenAIApiKey();\n",
      "\n",
      "  if (!openaiApiKey) {\n",
      "    throw new Error(msgs.apiKeyNotSet);\n",
      "  }\n",
      "\n",
      "  const openai = new OpenAIApi(new Configuration({ apiKey: openaiApiKey }));\n",
      "\n",
      "  const tokenSource = axios.CancelToken.source();\n",
      "  cancelToken.onCancellationRequested(tokenSource.cancel);\n",
      "\n",
      "  const nbMetadata = e.notebook.metadata.custom;\n",
      "  const defaultModel = workspace.getConfiguration().get<string>(\"notebook-chatcompletion.defaultModel\");\n",
      "  const model: TiktokenModel = nbMetadata?.model ?? defaultModel;\n",
      "  const temperature = nbMetadata?.temperature ?? 0;\n",
      "  const limit = getTokenLimit(model);\n",
      "\n",
      "  progress.report({ message: msgs.calculatingTokens, increment: 1 });\n",
      "  await waitForUIDispatch();\n",
      "\n",
      "  const msgText = JSON.stringify(messages);\n",
      "  const totalTokenCount = countTokens(msgText, model);\n",
      "\n",
      "  if (limit !== null && totalTokenCount > limit) {\n",
      "    const tokenOverflow = totalTokenCount - limit;\n",
      "\n",
      "    progress.report({ message: msgs.calculatingTokeReductions, increment: 1 });\n",
      "    await waitForUIDispatch();\n",
      "\n",
      "    const reducedMessages = await applyTokenReductions(messages, tokenOverflow, limit, model);\n",
      "\n",
      "    if (!reducedMessages) {\n",
      "      return FinishReason.cancelled;\n",
      "    }\n",
      "\n",
      "    messages = reducedMessages;\n",
      "  }\n",
      "\n",
      "  let reqParams: CreateChatCompletionRequest = {\n",
      "    model: model,\n",
      "    messages: messages,\n",
      "    stream: true,\n",
      "    temperature: temperature,\n",
      "  };\n",
      "\n",
      "  if (limit) {\n",
      "    const reducedMsgText = JSON.stringify(messages);\n",
      "    const reducedTokenCount = countTokens(reducedMsgText, model);\n",
      "    reqParams.max_tokens = limit - reducedTokenCount;\n",
      "\n",
      "    if (reqParams.max_tokens < 1) {\n",
      "      const result = await window.showInformationMessage(\n",
      "        `The request is estimated to be ${-reqParams.max_tokens} tokens over the limit (including the input) and will likely be rejected from the OpenAI API. Do you still want to proceed?`,\n",
      "        { modal: true },\n",
      "        \"Yes\"\n",
      "      );\n",
      "      if (result !== \"Yes\") {\n",
      "        return FinishReason.cancelled;\n",
      "      }\n",
      "      else{\n",
      "        \n",
      "        reqParams.max_tokens = undefined;\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "\n",
      "  reqParams = addNotebookConfigParams(nbMetadata, reqParams);\n",
      "\n",
      "  output.appendLine(\"\\n\" + JSON.stringify(reqParams, undefined, 2) + \"\\n\");\n",
      "  progress.report({ increment: 1, message: msgs.sendingRequest });\n",
      "\n",
      "  const response = await openai.createChatCompletion(reqParams, {\n",
      "    cancelToken: tokenSource.token,\n",
      "    responseType: \"stream\",\n",
      "  });\n",
      "\n",
      "  return await streamResponse(response, cancelToken, cellIndex, ck, progress);\n",
      "}\n",
      " \n",
      "\n",
      "\n",
      "File: '../../src/cellUtils.ts'\n",
      "\n",
      " import {\n",
      "  ChatCompletionRequestMessage,\n",
      "  ChatCompletionRequestMessageRoleEnum,\n",
      "} from \"openai\";\n",
      "import {\n",
      "  NotebookCellKind,\n",
      "  NotebookEdit,\n",
      "  NotebookEditor,\n",
      "  NotebookRange,\n",
      "  Range,\n",
      "  WorkspaceEdit,\n",
      "  commands,\n",
      "  languages,\n",
      "  window,\n",
      "  workspace,\n",
      "} from \"vscode\";\n",
      "import { CompletionType } from \"./completionType\";\n",
      "\n",
      "const ADDITIONAL_PROMPT_INFO_MESSAGE =\n",
      "  \"Select any additional information you want to include in the prompt\";\n",
      "\n",
      "export async function appendTextToCell(\n",
      "  editor: NotebookEditor,\n",
      "  cellIndex: number,\n",
      "  textToken: string\n",
      ") {\n",
      "  const existingCell = editor.notebook.cellAt(cellIndex);\n",
      "\n",
      "  \n",
      "  if (existingCell.document.getText().length === 0 && !/\\S/.test(textToken)) {\n",
      "    return;\n",
      "  }\n",
      "\n",
      "  if (textToken.startsWith(\"\\n\")) {\n",
      "    \n",
      "    if (existingCell.document.positionAt(1).character === 0) {\n",
      "      \n",
      "      if (textToken.length > 1) {\n",
      "        textToken = textToken.substring(1);\n",
      "      } else {\n",
      "        return;\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "\n",
      "  const edit = new WorkspaceEdit();\n",
      "  edit.insert(\n",
      "    existingCell.document.uri,\n",
      "    existingCell.document.positionAt(9999999999),\n",
      "    textToken\n",
      "  );\n",
      "  await workspace.applyEdit(edit);\n",
      "}\n",
      "\n",
      "export async function insertCell(\n",
      "  editor: NotebookEditor,\n",
      "  cellIndex: number,\n",
      "  cellKind: NotebookCellKind,\n",
      "  languageId: string = \"markdown\"\n",
      ") {\n",
      "  \n",
      "  const existingCell = editor.notebook.cellAt(cellIndex);\n",
      "  if (existingCell.document.getText().endsWith(\"\\n\")) {\n",
      "    const edit = new WorkspaceEdit();\n",
      "    const lastLineRange = existingCell.document.validateRange(\n",
      "      new Range(\n",
      "        existingCell.document.lineCount - 2,\n",
      "        9999999999,\n",
      "        existingCell.document.lineCount,\n",
      "        0\n",
      "      )\n",
      "    );\n",
      "\n",
      "    edit.delete(existingCell.document.uri, lastLineRange);\n",
      "    await workspace.applyEdit(edit);\n",
      "  }\n",
      "\n",
      "  await commands.executeCommand(\"notebook.cell.quitEdit\");\n",
      "\n",
      "  if (cellKind === NotebookCellKind.Code && languageId === \"python\") {\n",
      "    await commands.executeCommand(\"notebook.cell.insertCodeCellBelow\", [\n",
      "      { index: cellIndex },\n",
      "    ]);\n",
      "  } else {\n",
      "    await commands.executeCommand(\"notebook.cell.insertMarkdownCellBelow\", [\n",
      "      { index: cellIndex },\n",
      "    ]);\n",
      "  }\n",
      "\n",
      "  cellIndex++;\n",
      "\n",
      "  const edit = new WorkspaceEdit();\n",
      "  let cell = editor.notebook.cellAt(cellIndex);\n",
      "  edit.set(cell.notebook.uri, [\n",
      "    NotebookEdit.updateCellMetadata(cell.index, {\n",
      "      custom: { metadata: { tags: [\"assistant\"] } },\n",
      "    }),\n",
      "  ]);\n",
      "  await workspace.applyEdit(edit);\n",
      "\n",
      "  return cellIndex;\n",
      "}\n",
      "\n",
      "export async function convertCellsToMessages(\n",
      "  cellIndex: number,\n",
      "  completionType: CompletionType\n",
      "): Promise<ChatCompletionRequestMessage[]> {\n",
      "  const editor = window.activeNotebookEditor!;\n",
      "  const notebook = editor.notebook;\n",
      "\n",
      "  const startCellIndex =\n",
      "    completionType === CompletionType.currentCellAndAbove ? 0 : cellIndex;\n",
      "\n",
      "  const diagnostics = languages\n",
      "    .getDiagnostics()\n",
      "    .filter(([uri]) => uri.path === notebook.uri.path)\n",
      "    .flatMap(([, diag]) => diag);\n",
      "\n",
      "  const cellInfos = notebook\n",
      "    .getCells(new NotebookRange(startCellIndex, cellIndex + 1))\n",
      "    .map((cell) => {\n",
      "      const problems = diagnostics.filter(\n",
      "        (d) => cell.document.validateRange(d.range) === d.range\n",
      "      );\n",
      "      const nonImgOutputs = cell.outputs\n",
      "        .flatMap((o) => o.items.filter((i) => !i.mime.startsWith(\"image\")))\n",
      "        .map((i) => i.data.toString());\n",
      "      return { cell, problems, nonImgOutputs };\n",
      "    });\n",
      "\n",
      "  var messages: ChatCompletionRequestMessage[] = [];\n",
      "\n",
      "  cellInfos.forEach(({ cell, problems, nonImgOutputs }) => {\n",
      "    let role: ChatCompletionRequestMessageRoleEnum =\n",
      "      ChatCompletionRequestMessageRoleEnum.User;\n",
      "    const tags: string[] = cell.metadata?.custom?.metadata?.tags;\n",
      "\n",
      "    if (tags && tags.length > 0) {\n",
      "      role = tags[0] as ChatCompletionRequestMessageRoleEnum;\n",
      "    }\n",
      "\n",
      "    let cellContent = cell.document.getText();\n",
      "\n",
      "    if (cell.kind === NotebookCellKind.Code) {\n",
      "      cellContent = `\\`\\`\\`python \\n${cellContent}\\n\\`\\`\\``;\n",
      "    }\n",
      "\n",
      "    messages.push({ role: role, content: cellContent, name: cell.kind.toString()});\n",
      "\n",
      "    if (problems.length > 0) {\n",
      "      messages.push({\n",
      "        role: role ?? \"user\",\n",
      "        name: \"Problems\",\n",
      "        content:\n",
      "          \"Problems reported by VSCode from previous code:\\n\" +\n",
      "          problems.map((p) => `${p.code}: ${p.message}`),\n",
      "      });\n",
      "    }\n",
      "\n",
      "    if (nonImgOutputs.length) {\n",
      "      nonImgOutputs.forEach((output) =>\n",
      "        messages.push({\n",
      "          role: role ?? \"user\",\n",
      "          name: \"Output\",\n",
      "          content: \"Output from previous code:\\n\" + output,\n",
      "        })\n",
      "      );\n",
      "    }\n",
      "  });\n",
      "\n",
      "  const systemMessages = messages.filter(\n",
      "    (m) => m.role === ChatCompletionRequestMessageRoleEnum.System\n",
      "  );\n",
      "\n",
      "  \n",
      "  if (systemMessages.length === 0) {\n",
      "    messages.push({\n",
      "      role: ChatCompletionRequestMessageRoleEnum.System,\n",
      "      name: ChatCompletionRequestMessageRoleEnum.System,\n",
      "      content:\n",
      "        \"Format your answer as markdown. If you include a markdown code block, specify the language.\",\n",
      "    });\n",
      "  }\n",
      "\n",
      "  return messages;\n",
      "}\n",
      " \n",
      "\n",
      "\n",
      "File: '../../src/tokenUtils.ts'\n",
      "\n",
      " import { TiktokenModel, encoding_for_model } from \"@dqbd/tiktoken\";\n",
      "import { ChatCompletionRequestMessage as Message, ChatCompletionResponseMessageRoleEnum as Role } from \"openai\";\n",
      "import { QuickPickItem, window } from \"vscode\";\n",
      "import { msgs, uiText } from \"./constants\";\n",
      "\n",
      "function tabifyWhitespaces(message: Message): Message {\n",
      "  message.content = message.content.replace(/ {4}/g, \"\\t\");\n",
      "  return message;\n",
      "}\n",
      "\n",
      "export async function applyTokenReductions(\n",
      "  messages: Message[],\n",
      "  tokenOverflowCount: number,\n",
      "  limit: number,\n",
      "  model: TiktokenModel\n",
      "): Promise<Message[] | null> {\n",
      "  const replacements: { label: string; reduce: (arg1: Message) => Message | null }[] = [\n",
      "    { label: uiText.removeOutput, reduce: (m) => (m.name === \"Output\" ? null : m) },\n",
      "    { label: uiText.removeProblems, reduce: (m) => (m.name === \"Problems\" ? null : m) },\n",
      "    { label: uiText.removeSystemMsg, reduce: (m) => (m.role === Role.System ? null : m) },\n",
      "    { label: uiText.tabifyWhiteSpaces, reduce: tabifyWhitespaces },\n",
      "  ];\n",
      "\n",
      "  type TokenReductionStrategy = QuickPickItem & {\n",
      "    apply: Function;\n",
      "    savedTokens?: number;\n",
      "  };\n",
      "\n",
      "  let strategies: TokenReductionStrategy[] = replacements.map((strategy) => ({\n",
      "    label: strategy.label,\n",
      "    apply: () => messages.map(strategy.reduce).filter((x) => x !== null),\n",
      "  }));\n",
      "\n",
      "  const totalTokenCount = countTotalTokens(messages, model);\n",
      "\n",
      "  for (const strategy of strategies) {\n",
      "    const reducedMessages = await strategy.apply();\n",
      "    const reducedTokenCount = countTotalTokens(reducedMessages, model);\n",
      "    const savedTokens = totalTokenCount - reducedTokenCount;\n",
      "    strategy.savedTokens = savedTokens;\n",
      "    strategy.description = `${savedTokens} tokens`;\n",
      "  }\n",
      "\n",
      "  const maxPossibleSaving = strategies.map((x) => x.savedTokens ?? 0).reduce((prev, current) => prev + current);\n",
      "\n",
      "  if (maxPossibleSaving < tokenOverflowCount) {\n",
      "    window.showInformationMessage(\n",
      "      `If we applied every token reduction strategy available, you would still be ${\n",
      "        tokenOverflowCount - maxPossibleSaving\n",
      "      } over the limit of the '${model}' model. Please reduce the size of the content.`,\n",
      "      { modal: true }\n",
      "    );\n",
      "  }\n",
      "\n",
      "  const selectedStrategies = await window.showQuickPick(strategies, {\n",
      "    canPickMany: true,\n",
      "    title: `You have ${tokenOverflowCount} more tokens than the model's limit of ${limit}`,\n",
      "  });\n",
      "\n",
      "  if (!selectedStrategies) {\n",
      "    return null;\n",
      "  }\n",
      "\n",
      "  let reducedMessages = messages;\n",
      "  for (const strategy of selectedStrategies) {\n",
      "    reducedMessages = await strategy.apply(reducedMessages);\n",
      "  }\n",
      "\n",
      "  const reducedTokenCount = countTotalTokens(reducedMessages, model);\n",
      "  if (reducedTokenCount > limit) {\n",
      "    window.showErrorMessage(msgs.notEnoughSavings);\n",
      "    return null;\n",
      "  }\n",
      "\n",
      "  return reducedMessages;\n",
      "}\n",
      "\n",
      "export function countTokens(text: string, model: TiktokenModel): number {\n",
      "  const enc = encoding_for_model(model);\n",
      "  const tokenCount = enc.encode(text).length;\n",
      "  enc.free();\n",
      "  return tokenCount;\n",
      "}\n",
      "\n",
      "export function countTotalTokens(msgs: Message[], model: TiktokenModel): number {\n",
      "  return msgs.reduce((accumulator, message) => {\n",
      "    return accumulator + countTokens(message.content, model);\n",
      "  }, 0);\n",
      "}\n",
      " \n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import List\n",
    "import re\n",
    "\n",
    "def print_file_content(file_path: Path):\n",
    "    content = file_path.read_text()\n",
    "\n",
    "    # Remove single-line comments\n",
    "    content = re.sub(r\"//.*\", \"\", content)\n",
    "\n",
    "    # Remove multi-line comments\n",
    "    content = re.sub(r\"/\\*[\\s\\S]*?\\*/\", \"\", content)\n",
    "\n",
    "    print(f\"\\n\\nFile: '{file_path}'\")\n",
    "    print(\"\\n\", content, \"\")\n",
    "\n",
    "def process_files(file_patterns: List[str]):\n",
    "    for pattern in file_patterns:\n",
    "        for file in Path().glob(pattern):\n",
    "            print_file_content(file)\n",
    "\n",
    "file_patterns = [\"../../src/completion.ts\", \"../../src/cellUtils.ts\", \"../../src/tokenUtils.ts\"]\n",
    "process_files(file_patterns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is used in Python to estimate how many tokens the request message format from the OpenAI ChatCompletion message will take on top of the message content.\n",
    "\n",
    "```python\n",
    "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\"):\n",
    "    \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    if model == \"gpt-3.5-turbo\":\n",
    "        print(\"Warning: gpt-3.5-turbo may change over time. Returning num tokens assuming gpt-3.5-turbo-0301.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\")\n",
    "    elif model == \"gpt-4\":\n",
    "        print(\"Warning: gpt-4 may change over time. Returning num tokens assuming gpt-4-0314.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-4-0314\")\n",
    "    elif model == \"gpt-3.5-turbo-0301\":\n",
    "        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
    "        tokens_per_name = -1  # if there's a name, the role is omitted\n",
    "    elif model == \"gpt-4-0314\":\n",
    "        tokens_per_message = 3\n",
    "        tokens_per_name = 1\n",
    "    else:\n",
    "        raise NotImplementedError(f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\")\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "    return num_tokens\n",
    "```\n",
    "\n",
    "As you can see in the completion.ts from a VSCode extension, I am currently not counting tokens correctly. I am first converting the messages object to JSON and then counting the token of the resulting string, which deviates a lot from the real token count. Please correct the typescript implementation by taken reference from the python example with the num_tokens_from_messages function and apply the changes by overwriting the files like you did in Example 1 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "assistant"
    ]
   },
   "outputs": [],
   "source": [
    "with open('../../src/completion.ts', 'w') as f: f.write(\"\"\"\n",
    "import axios, { AxiosResponse } from \"axios\";\n",
    "import { Configuration, CreateChatCompletionRequest, CreateChatCompletionResponse, OpenAIApi } from \"openai\";\n",
    "import { CancellationToken, NotebookCellKind, NotebookEdit, NotebookRange, WorkspaceEdit, window, workspace } from \"vscode\";\n",
    "import { appendTextToCell, convertCellsToMessages, insertCell } from \"./cellUtils\";\n",
    "import { CompletionType } from \"./completionType\";\n",
    "import { addParametersFromMetadata as addNotebookConfigParams, getOpenAIApiKey, getTokenLimit } from \"./config\";\n",
    "import { msgs } from \"./constants\";\n",
    "import { FinishReason } from \"./finishReason\";\n",
    "import { bufferWholeChunks, streamChatCompletion } from \"./streamUtils\";\n",
    "import { applyTokenReductions, countTokens, countTotalTokens } from \"./tokenUtils\";\n",
    "import { UIProgress, waitForUIDispatch } from \"./uiProgress\";\n",
    "import { TiktokenModel } from \"@dqbd/tiktoken\";\n",
    "\n",
    "const output = window.createOutputChannel(\"Notebook ChatCompletion\");\n",
    "\n",
    "function numTokensFromMessages(messages: any[], model: string = \"gpt-3.5-turbo-0301\"): number {\n",
    "  const encoding = TiktokenModel.encoding_for_model(model);\n",
    "  let tokensPerMessage = 4;\n",
    "  let tokensPerName = -1;\n",
    "\n",
    "  let numTokens = 0;\n",
    "  for (const message of messages) {\n",
    "    numTokens += tokensPerMessage;\n",
    "    for (const key in message) {\n",
    "      const value = message[key];\n",
    "      numTokens += encoding.encode(value).length;\n",
    "      if (key === \"name\") {\n",
    "        numTokens += tokensPerName;\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  numTokens += 3;\n",
    "  return numTokens;\n",
    "}\n",
    "\n",
    "async function streamResponse(\n",
    "  response: AxiosResponse<CreateChatCompletionResponse, any>,\n",
    "  cancelToken: CancellationToken,\n",
    "  cellIndex: number,\n",
    "  ck: NotebookCellKind | undefined,\n",
    "  progress: UIProgress\n",
    ") {\n",
    "  const editor = window.activeNotebookEditor!;\n",
    "\n",
    "  for await (let textToken of bufferWholeChunks(streamChatCompletion(response, cancelToken))) {\n",
    "    if (Object.values(FinishReason).includes(textToken as FinishReason)) {\n",
    "      const currentCell = window.activeNotebookEditor!.notebook.cellAt(cellIndex);\n",
    "      const text = currentCell.document.getText();\n",
    "\n",
    "      if (!/\\S/.test(text)) {\n",
    "        const edit = new WorkspaceEdit();\n",
    "        edit.set(currentCell.notebook.uri, [NotebookEdit.deleteCells(new NotebookRange(currentCell.index, currentCell.index + 1))]);\n",
    "        await workspace.applyEdit(edit);\n",
    "      }\n",
    "\n",
    "      return textToken as FinishReason;\n",
    "    } else {\n",
    "      output.append(textToken.toString());\n",
    "    }\n",
    "\n",
    "    if (typeof textToken !== \"string\") {\n",
    "      throw new Error(`Unknown stream result: ${textToken}`);\n",
    "    }\n",
    "\n",
    "    if (textToken.includes(\"```python\\n\")) {\n",
    "      ck = NotebookCellKind.Code;\n",
    "\n",
    "      cellIndex = await insertCell(editor, cellIndex, ck, \"python\");\n",
    "      textToken = textToken.replace(\"```python\\n\", \"\");\n",
    "    } else if (textToken.includes(\"```\") && ck === NotebookCellKind.Code) {\n",
    "      textToken = textToken.replace(\"```\", \"\");\n",
    "\n",
    "      ck = NotebookCellKind.Markup;\n",
    "      cellIndex = await insertCell(editor, cellIndex, ck);\n",
    "    }\n",
    "\n",
    "    if (ck === undefined) {\n",
    "      cellIndex = await insertCell(editor, cellIndex, NotebookCellKind.Markup);\n",
    "      ck = NotebookCellKind.Markup;\n",
    "    }\n",
    "\n",
    "    await appendTextToCell(editor, cellIndex, textToken);\n",
    "\n",
    "    progress.report({ increment: 0.5, message: msgs.receivingTokens });\n",
    "  }\n",
    "\n",
    "  return FinishReason.length;\n",
    "}\n",
    "\n",
    "export async function generateCompletion(\n",
    "  cellIndex: number,\n",
    "  completionType: CompletionType,\n",
    "  progress: UIProgress,\n",
    "  cancelToken: CancellationToken\n",
    "): Promise<FinishReason> {\n",
    "  const e = window.activeNotebookEditor!;\n",
    "  let messages = await convertCellsToMessages(cellIndex, completionType);\n",
    "  let ck: NotebookCellKind | undefined = undefined;\n",
    "\n",
    "  const openaiApiKey = await getOpenAIApiKey();\n",
    "\n",
    "  if (!openaiApiKey) {\n",
    "    throw new Error(msgs.apiKeyNotSet);\n",
    "  }\n",
    "\n",
    "  const openai = new OpenAIApi(new Configuration({ apiKey: openaiApiKey }));\n",
    "\n",
    "  const tokenSource = axios.CancelToken.source();\n",
    "  cancelToken.onCancellationRequested(tokenSource.cancel);\n",
    "\n",
    "  const nbMetadata = e.notebook.metadata.custom;\n",
    "  const defaultModel = workspace.getConfiguration().get<string>(\"notebook-chatcompletion.defaultModel\");\n",
    "  const model: TiktokenModel = nbMetadata?.model ?? defaultModel;\n",
    "  const temperature = nbMetadata?.temperature ?? 0;\n",
    "  const limit = getTokenLimit(model);\n",
    "\n",
    "  progress.report({ message: msgs.calculatingTokens, increment: 1 });\n",
    "  await waitForUIDispatch();\n",
    "\n",
    "  const totalTokenCount = numTokensFromMessages(messages, model);\n",
    "\n",
    "  if (limit !== null && totalTokenCount > limit) {\n",
    "    const tokenOverflow = totalTokenCount - limit;\n",
    "\n",
    "    progress.report({ message: msgs.calculatingTokeReductions, increment: 1 });\n",
    "    await waitForUIDispatch();\n",
    "\n",
    "    const reducedMessages = await applyTokenReductions(messages, tokenOverflow, limit, model);\n",
    "\n",
    "    if (!reducedMessages) {\n",
    "      return FinishReason.cancelled;\n",
    "    }\n",
    "\n",
    "    messages = reducedMessages;\n",
    "  }\n",
    "\n",
    "  let reqParams: CreateChatCompletionRequest = {\n",
    "    model: model,\n",
    "    messages: messages,\n",
    "    stream: true,\n",
    "    temperature: temperature,\n",
    "  };\n",
    "\n",
    "  if (limit) {\n",
    "    const reducedTokenCount = numTokensFromMessages(messages, model);\n",
    "    reqParams.max_tokens = limit - reducedTokenCount;\n",
    "\n",
    "    if (reqParams.max_tokens < 1) {\n",
    "      const result = await window.showInformationMessage(\n",
    "        `The request is estimated to be ${-reqParams.max_tokens} tokens over the limit (including the input) and will likely be rejected from the OpenAI API. Do you still want to proceed?`,\n",
    "        { modal: true },\n",
    "        \"Yes\"\n",
    "      );\n",
    "      if (result !== \"Yes\") {\n",
    "        return FinishReason.cancelled;\n",
    "      }\n",
    "      else{\n",
    "        \n",
    "        reqParams.max_tokens = undefined;\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "\n",
    "  reqParams = addNotebookConfigParams(nbMetadata, reqParams);\n",
    "\n",
    "  output.appendLine(\"\\n\" + JSON.stringify(reqParams, undefined, 2) + \"\\n\");\n",
    "  progress.report({ increment: 1, message: msgs.sendingRequest });\n",
    "\n",
    "  const response = await openai.createChatCompletion(reqParams, {\n",
    "    cancelToken: tokenSource.token,\n",
    "    responseType: \"stream\",\n",
    "  });\n",
    "\n",
    "  return await streamResponse(response, cancelToken, cellIndex, ck, progress);\n",
    "}\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "model": "gpt-4",
 "nbformat": 4,
 "nbformat_minor": 2
}
