{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "system"
    ]
   },
   "source": [
    "> You are a code generator that can only answer with python code."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File: 'example.ts'\n",
    "\n",
    "```typescript\n",
    "function greet(name: string) {\n",
    "  console.log(\"Hello, \" + name);\n",
    "}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add the age of the user to the console log.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": [
     "assistant"
    ]
   },
   "outputs": [],
   "source": [
    "with open('example.ts', 'w') as f: f.write(\"\"\"\n",
    "function greet(name: string, age : number) {\n",
    "  console.log(`Hello, ${name}. You are ${age} years old.`);\n",
    "}\n",
    "\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File: '../../src/finishReason.ts'\n",
    "\n",
    "```typescript\n",
    "export enum FinishReason {\n",
    "  length,\n",
    "  contentFilter,\n",
    "  stop,\n",
    "  null,\n",
    "  cancelled\n",
    "}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**My compiler says that FinishReason \"timeout\" is missing. Fix the issue.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "assistant"
    ]
   },
   "outputs": [],
   "source": [
    "with open('../../src/finishReason.ts', 'w') as f: f.write(\"\"\"\n",
    "export enum FinishReason {\n",
    "  length,\n",
    "  contentFilter,\n",
    "  stop,\n",
    "  null,\n",
    "  cancelled,\n",
    "  timeout\n",
    "}\n",
    "\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "File: '../../src/completion.ts'\n",
      "\n",
      " \n",
      "import axios from \"axios\";\n",
      "import { ChatCompletionRequestMessage, Configuration, CreateChatCompletionRequest, OpenAIApi } from \"openai\";\n",
      "import {\n",
      "  CancellationToken,\n",
      "  ConfigurationTarget,\n",
      "  NotebookCellKind,\n",
      "  NotebookEdit,\n",
      "  NotebookRange,\n",
      "  QuickPickItem,\n",
      "  WorkspaceEdit,\n",
      "  window,\n",
      "  workspace,\n",
      "} from \"vscode\";\n",
      "import { appendTextToCell, convertCellsToMessages, insertCell } from \"./cellUtils\";\n",
      "import { CompletionType } from \"./completionType\";\n",
      "import { FinishReason } from \"./finishReason\";\n",
      "import { bufferWholeChunks, streamChatCompletion } from \"./streamUtils\";\n",
      "import { UIProgress } from \"./uiProgress\";\n",
      "import { encoding_for_model } from \"@dqbd/tiktoken\";\n",
      "\n",
      "const output = window.createOutputChannel(\"Notebook ChatCompletion\");\n",
      "\n",
      "export async function generateCompletion(\n",
      "  ci: number,\n",
      "  ct: CompletionType,\n",
      "  p: UIProgress,\n",
      "  t: CancellationToken,\n",
      "  prevFR: FinishReason\n",
      "): Promise<FinishReason> {\n",
      "  const e = window.activeNotebookEditor!;\n",
      "  let msgs = await convertCellsToMessages(ci, ct);\n",
      "  let ck: NotebookCellKind | undefined = undefined;\n",
      "\n",
      "  const openaiApiKey = await getOpenAIApiKey();\n",
      "\n",
      "  if (!openaiApiKey) {\n",
      "    throw new Error(\"OpenAI API key is not set\");\n",
      "  }\n",
      "\n",
      "  const openai = new OpenAIApi(new Configuration({ apiKey: openaiApiKey }));\n",
      "\n",
      "  const tokenSource = axios.CancelToken.source();\n",
      "  t.onCancellationRequested(() => tokenSource.cancel());\n",
      "\n",
      "  const nbMetadata = e.notebook.metadata.custom;\n",
      "\n",
      "  const defaultModel = workspace.getConfiguration().get<string>(\"notebook-chatcompletion.defaultModel\");\n",
      "\n",
      "  const model = nbMetadata?.model ?? defaultModel;\n",
      "  const temperature = nbMetadata?.temperature ?? 0;\n",
      "\n",
      "  let limit: number | null = null;\n",
      "\n",
      "  switch (model) {\n",
      "    case \"gpt-4\":\n",
      "    case \"gpt-4-0314\":\n",
      "      limit = 8192;\n",
      "      break;\n",
      "\n",
      "    case \"gpt-4-32k\":\n",
      "    case \"gpt-4-32k-0314\":\n",
      "      limit = 32768;\n",
      "      break;\n",
      "\n",
      "    case \"gpt-3.5-turbo\":\n",
      "    case \"gpt-3.5-turbo-0301\":\n",
      "      limit = 4096;\n",
      "      break;\n",
      "\n",
      "    default:\n",
      "      break;\n",
      "  }\n",
      "\n",
      "  const msgText = JSON.stringify(msgs);\n",
      "  const totalTokenCount = countTokens(msgText, model);\n",
      "\n",
      "  if (limit !== null && totalTokenCount > limit) {\n",
      "    const tokenOverflow = limit - totalTokenCount;\n",
      "\n",
      "    const msgText = msgs.map((x) => x.content).join();\n",
      "    const contentTokenCount = countTokens(msgText, model);\n",
      "\n",
      "    const reducedMessages = await applyTokenReductionStrategies(msgs, tokenOverflow, contentTokenCount, limit, model);\n",
      "\n",
      "    if (!reducedMessages) {\n",
      "      return FinishReason.cancelled;\n",
      "    }\n",
      "\n",
      "    msgs = reducedMessages;\n",
      "  }\n",
      "\n",
      "  let reqParams: CreateChatCompletionRequest = {\n",
      "    model: model,\n",
      "    messages: msgs,\n",
      "    stream: true,\n",
      "    temperature: temperature,\n",
      "  };\n",
      "\n",
      "  if (limit) {\n",
      "    const reducedMsgText = JSON.stringify(msgs);\n",
      "    const reducedTokenCount = countTokens(reducedMsgText, model);\n",
      "    reqParams.max_tokens = limit - reducedTokenCount;\n",
      "\n",
      "    if (reqParams.max_tokens < 1) {\n",
      "      const result = await window.showInformationMessage(\n",
      "        `The request is estimated to be ${-reqParams.max_tokens} tokens over the limit (including tokens consumed by the request format) and will likely be rejected from the OpenAI API. Do you still want to proceed?`,\n",
      "        { modal: true },\n",
      "        \"Yes\"\n",
      "      );\n",
      "      if (result !== \"Yes\") {\n",
      "        return FinishReason.cancelled;\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "\n",
      "  reqParams = addParametersFromMetadata(nbMetadata, reqParams);\n",
      "\n",
      "  output.appendLine(\"\\n\" + JSON.stringify(reqParams, undefined, 2) + \"\\n\");\n",
      "  p.report({ increment: 1, message: \"Sending ChatCompletion request\" });\n",
      "\n",
      "  const response = await openai.createChatCompletion(reqParams, {\n",
      "    cancelToken: tokenSource.token,\n",
      "    responseType: \"stream\",\n",
      "  });\n",
      "\n",
      "  for await (let textToken of bufferWholeChunks(streamChatCompletion(response, t))) {\n",
      "    if (Object.values(FinishReason).includes(textToken as FinishReason)) {\n",
      "      switch (textToken) {\n",
      "        case FinishReason.length:\n",
      "          output.append(\"FINISH_REASON_LENGTH\" + \"\\n\");\n",
      "          break;\n",
      "        case FinishReason.contentFilter:\n",
      "          output.append(\"FINISH_REASON_CONTENTFILTER\" + \"\\n\");\n",
      "          break;\n",
      "        case FinishReason.stop:\n",
      "          output.append(\"FINISH_REASON_STOP\" + \"\\n\");\n",
      "          break;\n",
      "      }\n",
      "\n",
      "      const currentCell = e.notebook.cellAt(ci);\n",
      "      const text = currentCell.document.getText();\n",
      "\n",
      "      if (!/\\S/.test(text)) {\n",
      "        const edit = new WorkspaceEdit();\n",
      "        edit.set(currentCell.notebook.uri, [NotebookEdit.deleteCells(new NotebookRange(currentCell.index, currentCell.index + 1))]);\n",
      "        await workspace.applyEdit(edit);\n",
      "      }\n",
      "\n",
      "      return textToken as FinishReason;\n",
      "    } else {\n",
      "      output.append(textToken.toString());\n",
      "    }\n",
      "\n",
      "    if (typeof textToken !== \"string\") {\n",
      "      throw new Error(\"Invalid state: unknown stream result: \" + textToken);\n",
      "    }\n",
      "\n",
      "    if (textToken.includes(\"```python\\n\")) {\n",
      "      ck = NotebookCellKind.Code;\n",
      "\n",
      "      ci = await insertCell(e, ci, ck, \"python\");\n",
      "      textToken = textToken.replace(\"```python\\n\", \"\");\n",
      "    } else if (textToken.includes(\"```\") && ck === NotebookCellKind.Code) {\n",
      "      textToken = textToken.replace(\"```\", \"\");\n",
      "\n",
      "      ck = NotebookCellKind.Markup;\n",
      "      ci = await insertCell(e, ci, ck, \"markdown\");\n",
      "    }\n",
      "\n",
      "    if (ck === undefined) {\n",
      "      ci = await insertCell(e, ci, NotebookCellKind.Markup, \"markdown\");\n",
      "      ck = NotebookCellKind.Markup;\n",
      "    }\n",
      "\n",
      "    await appendTextToCell(e, ci, textToken);\n",
      "\n",
      "    p.report({ increment: 0.5, message: \"Receiving tokens...\" });\n",
      "  }\n",
      "\n",
      "  return FinishReason.length;\n",
      "}\n",
      "\n",
      "function addParametersFromMetadata(nbMetadata: any, reqParams: CreateChatCompletionRequest) {\n",
      "  const e = window.activeNotebookEditor;\n",
      "  if (e && nbMetadata) {\n",
      "    if (e.notebook.metadata.custom?.top_p) {\n",
      "      reqParams = {\n",
      "        ...reqParams,\n",
      "        top_p: e.notebook.metadata.custom.top_p,\n",
      "      };\n",
      "    }\n",
      "    if (e.notebook.metadata.custom?.n) {\n",
      "      reqParams = {\n",
      "        ...reqParams,\n",
      "        n: e.notebook.metadata.custom.n,\n",
      "      };\n",
      "    }\n",
      "    if (e.notebook.metadata.custom?.max_tokens) {\n",
      "      reqParams.max_tokens = e.notebook.metadata.custom.max_tokens;\n",
      "    }\n",
      "    if (e.notebook.metadata.custom?.presence_penalty) {\n",
      "      reqParams = {\n",
      "        ...reqParams,\n",
      "        presence_penalty: e.notebook.metadata.custom.presence_penalty,\n",
      "      };\n",
      "    }\n",
      "    if (e.notebook.metadata.custom?.frequency_penalty) {\n",
      "      reqParams = {\n",
      "        ...reqParams,\n",
      "        frequency_penalty: e.notebook.metadata.custom.frequency_penalty,\n",
      "      };\n",
      "    }\n",
      "    if (e.notebook.metadata.custom?.logit_bias) {\n",
      "      reqParams = {\n",
      "        ...reqParams,\n",
      "        logit_bias: e.notebook.metadata.custom.logit_bias,\n",
      "      };\n",
      "    }\n",
      "    if (e.notebook.metadata.custom?.user) {\n",
      "      reqParams = {\n",
      "        ...reqParams,\n",
      "        user: e.notebook.metadata.custom.top_p,\n",
      "      };\n",
      "    }\n",
      "  }\n",
      "  return reqParams;\n",
      "}\n",
      "\n",
      "async function getOpenAIApiKey(): Promise<string> {\n",
      "  let apiKey = workspace.getConfiguration().get<string>(\"notebook-chatcompletion.openaiApiKey\");\n",
      "  if (!apiKey) {\n",
      "    apiKey = await window.showInputBox({\n",
      "      prompt: \"Enter your OpenAI API Key:\",\n",
      "      validateInput: (value) => (value.trim().length > 0 ? null : \"API Key cannot be empty\"),\n",
      "    });\n",
      "\n",
      "    if (apiKey) {\n",
      "      await workspace.getConfiguration().update(\"notebook-chatcompletion.openaiApiKey\", apiKey, ConfigurationTarget.Global);\n",
      "\n",
      "      await window.showInformationMessage(\n",
      "        \"Please note that the model is set to GPT-4 by default, which you may not be able to access yet. As a result, the API may return an HTTP 404 error. You can change the 'Default Model' setting to another model or use the 'Set Model' command in the menu to set the model for a specific notebook.\",\n",
      "        { modal: true }\n",
      "      );\n",
      "    } else {\n",
      "      window.showErrorMessage(\"OpenAI API Key is required for Notebook ChatCompletion to work.\", { modal: true });\n",
      "      return \"\";\n",
      "    }\n",
      "  }\n",
      "\n",
      "  return apiKey;\n",
      "}\n",
      "\n",
      "type TokenReductionStrategy = QuickPickItem & {\n",
      "  apply: Function;\n",
      "  savedTokens?: number;\n",
      "};\n",
      "\n",
      "async function applyTokenReductionStrategies(\n",
      "  msgs: ChatCompletionRequestMessage[],\n",
      "  tokenOverflowCount: number,\n",
      "  totalTokenCount: number,\n",
      "  limit: number,\n",
      "  model: string\n",
      "): Promise<ChatCompletionRequestMessage[] | null> {\n",
      "  let strategies: TokenReductionStrategy[] = [\n",
      "    {\n",
      "      label: \"Remove all Cell Output\",\n",
      "      apply: async () => {\n",
      "        return msgs.filter((message) => !message.content.startsWith(\"Output from previous code:\"));\n",
      "      },\n",
      "    },\n",
      "    {\n",
      "      label: \"Remove all VSCode Problems\",\n",
      "      apply: async () => {\n",
      "        return msgs.filter((message) => !message.content.startsWith(\"Problems reported by VSCode from previous code:\"));\n",
      "      },\n",
      "    },\n",
      "    {\n",
      "      label: \"Remove Spaces\",\n",
      "      apply: async () => {\n",
      "        return msgs.map((message) => ({\n",
      "          ...message,\n",
      "          content: message.content.replace(/ /g, \"\"),\n",
      "        }));\n",
      "      },\n",
      "    },\n",
      "    {\n",
      "      label: \"Remove Line-breaks\",\n",
      "      apply: async () => {\n",
      "        return msgs.map((message) => ({\n",
      "          ...message,\n",
      "          content: message.content.replace(/\\n/g, \"\"),\n",
      "        }));\n",
      "      },\n",
      "    },\n",
      "    {\n",
      "      label: \"Remove Punctuations\",\n",
      "      apply: async () => {\n",
      "        return msgs.map((message) => ({\n",
      "          ...message,\n",
      "          content: message.content.replace(/[.,;:!?]/g, \"\"),\n",
      "        }));\n",
      "      },\n",
      "    },\n",
      "  ];\n",
      "\n",
      "  for (const strategy of strategies) {\n",
      "    const reducedMessages = await strategy.apply();\n",
      "    const reducedTokenCount = countTotalTokens(reducedMessages, model);\n",
      "    const savedTokens = totalTokenCount - reducedTokenCount;\n",
      "    strategy.savedTokens = savedTokens;\n",
      "    strategy.description = `${savedTokens} tokens`;\n",
      "  }\n",
      "\n",
      "  strategies = strategies.filter((s) => (s.savedTokens ? s.savedTokens > 1 : false));\n",
      "\n",
      "  const maxPossibleSaving = strategies.map((x) => x.savedTokens ?? 0).reduce((prev, current) => prev + current);\n",
      "\n",
      "  if (maxPossibleSaving < tokenOverflowCount) {\n",
      "    window.showInformationMessage(\n",
      "      `If we applied every token reduction strategy available, you would still be ${\n",
      "        tokenOverflowCount - maxPossibleSaving\n",
      "      } over the limit of the '${model}' model. Please reduce the size of the content.`,\n",
      "      { modal: true }\n",
      "    );\n",
      "  }\n",
      "\n",
      "  const selectedStrategies = await window.showQuickPick(strategies, {\n",
      "    canPickMany: true,\n",
      "    title: \"Too many tokens\",\n",
      "    placeHolder: \"Select one or more strategies to reduce the token count\",\n",
      "  });\n",
      "\n",
      "  if (!selectedStrategies) {\n",
      "    return null;\n",
      "  }\n",
      "\n",
      "  let reducedMessages = msgs;\n",
      "  for (const strategy of selectedStrategies) {\n",
      "    reducedMessages = await strategy.apply(reducedMessages);\n",
      "  }\n",
      "\n",
      "  const reducedTokenCount = countTotalTokens(reducedMessages, model);\n",
      "  if (reducedTokenCount > limit) {\n",
      "    window.showErrorMessage(\"The selected strategies do not reduce tokens below the limit.\");\n",
      "    return null;\n",
      "  }\n",
      "\n",
      "  return reducedMessages;\n",
      "}\n",
      "\n",
      "function countTokens(text: string, model: any): number {\n",
      "  const enc = encoding_for_model(model);\n",
      "  const tokenCount = enc.encode(text).length;\n",
      "  enc.free();\n",
      "  return tokenCount;\n",
      "}\n",
      "\n",
      "function countTotalTokens(msgs: ChatCompletionRequestMessage[], model: string): number {\n",
      "  return msgs.reduce((accumulator, message) => {\n",
      "    return accumulator + countTokens(message.content, model);\n",
      "  }, 0);\n",
      "}\n",
      " \n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import List\n",
    "import re\n",
    "\n",
    "def print_file_content(file_path: Path):\n",
    "    content = file_path.read_text()\n",
    "\n",
    "    # Remove single-line comments\n",
    "    content = re.sub(r\"//.*\", \"\", content)\n",
    "\n",
    "    # Remove multi-line comments\n",
    "    content = re.sub(r\"/\\*[\\s\\S]*?\\*/\", \"\", content)\n",
    "\n",
    "    print(f\"\\n\\nFile: '{file_path}'\")\n",
    "    print(\"\\n\", content, \"\")\n",
    "\n",
    "def process_files(file_patterns: List[str]):\n",
    "    for pattern in file_patterns:\n",
    "        for file in Path().glob(pattern):\n",
    "            print_file_content(file)\n",
    "\n",
    "file_patterns = [\"../../src/completion.ts\"]\n",
    "process_files(file_patterns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Refactor all the strings into constants in the global scope and overwrite the file with the changes like you did in Example 1 and 2.**\n",
    "**For example, \"OpenAI API key is not set\" is moved to const msgs = { apiKeyNotSet: \"OpenAI API key is not set\", ... }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "assistant"
    ]
   },
   "outputs": [],
   "source": [
    "with open('../../src/completion.ts', 'w') as f: f.write(\"\"\"\n",
    "import axios from \"axios\";\n",
    "import { ChatCompletionRequestMessage, Configuration, CreateChatCompletionRequest, OpenAIApi } from \"openai\";\n",
    "import {\n",
    "  CancellationToken,\n",
    "  ConfigurationTarget,\n",
    "  NotebookCellKind,\n",
    "  NotebookEdit,\n",
    "  NotebookRange,\n",
    "  QuickPickItem,\n",
    "  WorkspaceEdit,\n",
    "  window,\n",
    "  workspace,\n",
    "} from \"vscode\";\n",
    "import { appendTextToCell, convertCellsToMessages, insertCell } from \"./cellUtils\";\n",
    "import { CompletionType } from \"./completionType\";\n",
    "import { FinishReason } from \"./finishReason\";\n",
    "import { bufferWholeChunks, streamChatCompletion } from \"./streamUtils\";\n",
    "import { UIProgress } from \"./uiProgress\";\n",
    "import { encoding_for_model } from \"@dqbd/tiktoken\";\n",
    "\n",
    "const output = window.createOutputChannel(\"Notebook ChatCompletion\");\n",
    "\n",
    "const msgs = {\n",
    "  apiKeyNotSet: \"OpenAI API key is not set\",\n",
    "  enterApiKey: \"Enter your OpenAI API Key:\",\n",
    "  apiKeyCannotBeEmpty: \"API Key cannot be empty\",\n",
    "  modelNotAccessible: \"Please note that the model is set to GPT-4 by default, which you may not be able to access yet. As a result, the API may return an HTTP 404 error. You can change the 'Default Model' setting to another model or use the 'Set Model' command in the menu to set the model for a specific notebook.\",\n",
    "  apiKeyRequired: \"OpenAI API Key is required for Notebook ChatCompletion to work.\",\n",
    "};\n",
    "\n",
    "export async function generateCompletion(\n",
    "  ci: number,\n",
    "  ct: CompletionType,\n",
    "  p: UIProgress,\n",
    "  t: CancellationToken,\n",
    "  prevFR: FinishReason\n",
    "): Promise<FinishReason> {\n",
    "  const e = window.activeNotebookEditor!;\n",
    "  let msgs = await convertCellsToMessages(ci, ct);\n",
    "  let ck: NotebookCellKind | undefined = undefined;\n",
    "\n",
    "  const openaiApiKey = await getOpenAIApiKey();\n",
    "\n",
    "  if (!openaiApiKey) {\n",
    "    throw new Error(msgs.apiKeyNotSet);\n",
    "  }\n",
    "\n",
    "  const openai = new OpenAIApi(new Configuration({ apiKey: openaiApiKey }));\n",
    "\n",
    "  const tokenSource = axios.CancelToken.source();\n",
    "  t.onCancellationRequested(() => tokenSource.cancel());\n",
    "\n",
    "  const nbMetadata = e.notebook.metadata.custom;\n",
    "\n",
    "  const defaultModel = workspace.getConfiguration().get<string>(\"notebook-chatcompletion.defaultModel\");\n",
    "\n",
    "  const model = nbMetadata?.model ?? defaultModel;\n",
    "  const temperature = nbMetadata?.temperature ?? 0;\n",
    "\n",
    "  let limit: number | null = null;\n",
    "\n",
    "  switch (model) {\n",
    "    case \"gpt-4\":\n",
    "    case \"gpt-4-0314\":\n",
    "      limit = 8192;\n",
    "      break;\n",
    "\n",
    "    case \"gpt-4-32k\":\n",
    "    case \"gpt-4-32k-0314\":\n",
    "      limit = 32768;\n",
    "      break;\n",
    "\n",
    "    case \"gpt-3.5-turbo\":\n",
    "    case \"gpt-3.5-turbo-0301\":\n",
    "      limit = 4096;\n",
    "      break;\n",
    "\n",
    "    default:\n",
    "      break;\n",
    "  }\n",
    "\n",
    "  const msgText = JSON.stringify(msgs);\n",
    "  const totalTokenCount = countTokens(msgText, model);\n",
    "\n",
    "  if (limit !== null && totalTokenCount > limit) {\n",
    "    const tokenOverflow = limit - totalTokenCount;\n",
    "\n",
    "    const msgText = msgs.map((x) => x.content).join();\n",
    "    const contentTokenCount = countTokens(msgText, model);\n",
    "\n",
    "    const reducedMessages = await applyTokenReductionStrategies(msgs, tokenOverflow, contentTokenCount, limit, model);\n",
    "\n",
    "    if (!reducedMessages) {\n",
    "      return FinishReason.cancelled;\n",
    "    }\n",
    "\n",
    "    msgs = reducedMessages;\n",
    "  }\n",
    "\n",
    "  let reqParams: CreateChatCompletionRequest = {\n",
    "    model: model,\n",
    "    messages: msgs,\n",
    "    stream: true,\n",
    "    temperature: temperature,\n",
    "  };\n",
    "\n",
    "  if (limit) {\n",
    "    const reducedMsgText = JSON.stringify(msgs);\n",
    "    const reducedTokenCount = countTokens(reducedMsgText, model);\n",
    "    reqParams.max_tokens = limit - reducedTokenCount;\n",
    "\n",
    "    if (reqParams.max_tokens < 1) {\n",
    "      const result = await window.showInformationMessage(\n",
    "        `The request is estimated to be ${-reqParams.max_tokens} tokens over the limit (including tokens consumed by the request format) and will likely be rejected from the OpenAI API. Do you still want to proceed?`,\n",
    "        { modal: true },\n",
    "        \"Yes\"\n",
    "      );\n",
    "      if (result !== \"Yes\") {\n",
    "        return FinishReason.cancelled;\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "\n",
    "  reqParams = addParametersFromMetadata(nbMetadata, reqParams);\n",
    "\n",
    "  output.appendLine(\"\\n\" + JSON.stringify(reqParams, undefined, 2) + \"\\n\");\n",
    "  p.report({ increment: 1, message: \"Sending ChatCompletion request\" });\n",
    "\n",
    "  const response = await openai.createChatCompletion(reqParams, {\n",
    "    cancelToken: tokenSource.token,\n",
    "    responseType: \"stream\",\n",
    "  });\n",
    "\n",
    "  for await (let textToken of bufferWholeChunks(streamChatCompletion(response, t))) {\n",
    "    if (Object.values(FinishReason).includes(textToken as FinishReason)) {\n",
    "      switch (textToken) {\n",
    "        case FinishReason.length:\n",
    "          output.append(\"FINISH_REASON_LENGTH\" + \"\\n\");\n",
    "          break;\n",
    "        case FinishReason.contentFilter:\n",
    "          output.append(\"FINISH_REASON_CONTENTFILTER\" + \"\\n\");\n",
    "          break;\n",
    "        case FinishReason.stop:\n",
    "          output.append(\"FINISH_REASON_STOP\" + \"\\n\");\n",
    "          break;\n",
    "      }\n",
    "\n",
    "      const currentCell = e.notebook.cellAt(ci);\n",
    "      const text = currentCell.document.getText();\n",
    "\n",
    "      if (!/\\S/.test(text)) {\n",
    "        const edit = new WorkspaceEdit();\n",
    "        edit.set(currentCell.notebook.uri, [NotebookEdit.deleteCells(new NotebookRange(currentCell.index, currentCell.index + 1))]);\n",
    "        await workspace.applyEdit(edit);\n",
    "      }\n",
    "\n",
    "      return textToken as FinishReason;\n",
    "    } else {\n",
    "      output.append(textToken.toString());\n",
    "    }\n",
    "\n",
    "    if (typeof textToken !== \"string\") {\n",
    "      throw new Error(\"Invalid state: unknown stream result: \" + textToken);\n",
    "    }\n",
    "\n",
    "    if (textToken.includes(\"```python\\n\")) {\n",
    "      ck = NotebookCellKind.Code;\n",
    "\n",
    "      ci = await insertCell(e, ci, ck, \"python\");\n",
    "      textToken = textToken.replace(\"```python\\n\", \"\");\n",
    "    } else if (textToken.includes(\"```\") && ck === NotebookCellKind.Code) {\n",
    "      textToken = textToken.replace(\"```\", \"\");\n",
    "\n",
    "      ck = NotebookCellKind.Markup;\n",
    "      ci = await insertCell(e, ci, ck, \"markdown\");\n",
    "    }\n",
    "\n",
    "    if (ck === undefined) {\n",
    "      ci = await insertCell(e, ci, NotebookCellKind.Markup, \"markdown\");\n",
    "      ck = NotebookCellKind.Markup;\n",
    "    }\n",
    "\n",
    "    await appendTextToCell(e, ci, textToken);\n",
    "\n",
    "    p.report({ increment: 0.5, message: \"Receiving tokens...\" });\n",
    "  }\n",
    "\n",
    "  return FinishReason.length;\n",
    "}\n",
    "\n",
    "function addParametersFromMetadata(nbMetadata: any, reqParams: CreateChatCompletionRequest) {\n",
    "  const e = window.activeNotebookEditor;\n",
    "  if (e && nbMetadata) {\n",
    "    if (e.notebook.metadata.custom?.top_p) {\n",
    "      reqParams = {\n",
    "        ...reqParams,\n",
    "        top_p: e.notebook.metadata.custom.top_p,\n",
    "      };\n",
    "    }\n",
    "    if (e.notebook.metadata.custom?.n) {\n",
    "      reqParams = {\n",
    "        ...reqParams,\n",
    "        n: e.notebook.metadata.custom.n,\n",
    "      };\n",
    "    }\n",
    "    if (e.notebook.metadata.custom?.max_tokens) {\n",
    "      reqParams.max_tokens = e.notebook.metadata.custom.max_tokens;\n",
    "    }\n",
    "    if (e.notebook.metadata.custom?.presence_penalty) {\n",
    "      reqParams = {\n",
    "        ...reqParams,\n",
    "        presence_penalty: e.notebook.metadata.custom.presence_penalty,\n",
    "      };\n",
    "    }\n",
    "    if (e.notebook.metadata.custom?.frequency_penalty) {\n",
    "      reqParams = {\n",
    "        ...reqParams,\n",
    "        frequency_penalty: e.notebook.metadata.custom.frequency_penalty,\n",
    "      };\n",
    "    }\n",
    "    if (e.notebook.metadata.custom?.logit_bias) {\n",
    "      reqParams = {\n",
    "        ...reqParams,\n",
    "        logit_bias: e.notebook.metadata.custom.logit_bias,\n",
    "      };\n",
    "    }\n",
    "    if (e.notebook.metadata.custom?.user) {\n",
    "      reqParams = {\n",
    "        ...reqParams,\n",
    "        user: e.notebook.metadata.custom.top_p,\n",
    "      };\n",
    "    }\n",
    "  }\n",
    "  return reqParams;\n",
    "}\n",
    "\n",
    "async function getOpenAIApiKey(): Promise<string> {\n",
    "  let apiKey = workspace.getConfiguration().get<string>(\"notebook-chatcompletion.openaiApiKey\");\n",
    "  if (!apiKey) {\n",
    "    apiKey = await window.showInputBox({\n",
    "      prompt: msgs.enterApiKey,\n",
    "      validateInput: (value) => (value.trim().length > 0 ? null : msgs.apiKeyCannotBeEmpty),\n",
    "    });\n",
    "\n",
    "    if (apiKey) {\n",
    "      await workspace.getConfiguration().update(\"notebook-chatcompletion.openaiApiKey\", apiKey, ConfigurationTarget.Global);\n",
    "\n",
    "      await window.showInformationMessage(\n",
    "        msgs.modelNotAccessible,\n",
    "        { modal: true }\n",
    "      );\n",
    "    } else {\n",
    "      window.showErrorMessage(msgs.apiKeyRequired, { modal: true });\n",
    "      return \"\";\n",
    "    }\n",
    "  }\n",
    "\n",
    "  return apiKey;\n",
    "}\n",
    "\n",
    "type TokenReductionStrategy = QuickPickItem & {\n",
    "  apply: Function;\n",
    "  savedTokens?: number;\n",
    "};\n",
    "\n",
    "async function applyTokenReductionStrategies(\n",
    "  msgs: ChatCompletionRequestMessage[],\n",
    "  tokenOverflowCount: number,\n",
    "  totalTokenCount: number,\n",
    "  limit: number,\n",
    "  model: string\n",
    "): Promise<ChatCompletionRequestMessage[] | null> {\n",
    "  let strategies: TokenReductionStrategy[] = [\n",
    "    {\n",
    "      label: \"Remove all Cell Output\",\n",
    "      apply: async () => {\n",
    "        return msgs.filter((message) => !message.content.startsWith(\"Output from previous code:\"));\n",
    "      },\n",
    "    },\n",
    "    {\n",
    "      label: \"Remove all VSCode Problems\",\n",
    "      apply: async () => {\n",
    "        return msgs.filter((message) => !message.content.startsWith(\"Problems reported by VSCode from previous code:\"));\n",
    "      },\n",
    "    },\n",
    "    {\n",
    "      label: \"Remove Spaces\",\n",
    "      apply: async () => {\n",
    "        return msgs.map((message) => ({\n",
    "          ...message,\n",
    "          content: message.content.replace(/ /g, \"\"),\n",
    "        }));\n",
    "      },\n",
    "    },\n",
    "    {\n",
    "      label: \"Remove Line-breaks\",\n",
    "      apply: async () => {\n",
    "        return msgs.map((message) => ({\n",
    "          ...message,\n",
    "          content: message.content.replace(/\\n/g, \"\"),\n",
    "        }));\n",
    "      },\n",
    "    },\n",
    "    {\n",
    "      label: \"Remove Punctuations\",\n",
    "      apply: async () => {\n",
    "        return msgs.map((message) => ({\n",
    "          ...message,\n",
    "          content: message.content.replace(/[.,;:!?]/g, \"\"),\n",
    "        }));\n",
    "      },\n",
    "    },\n",
    "  ];\n",
    "\n",
    "  for (const strategy of strategies) {\n",
    "    const reducedMessages = await strategy.apply();\n",
    "    const reducedTokenCount = countTotalTokens(reducedMessages, model);\n",
    "    const savedTokens = totalTokenCount - reducedTokenCount;\n",
    "    strategy.savedTokens = savedTokens;\n",
    "    strategy.description = `${savedTokens} tokens`;\n",
    "  }\n",
    "\n",
    "  strategies = strategies.filter((s) => (s.savedTokens ? s.savedTokens > 1 : false));\n",
    "\n",
    "  const maxPossibleSaving = strategies.map((x) => x.savedTokens ?? 0).reduce((prev, current) => prev + current);\n",
    "\n",
    "  if (maxPossibleSaving < tokenOverflowCount) {\n",
    "    window.showInformationMessage(\n",
    "      `If we applied every token reduction strategy available, you would still be ${\n",
    "        tokenOverflowCount - maxPossibleSaving\n",
    "      } over the limit of the '${model}' model. Please reduce the size of the content.`,\n",
    "      { modal: true }\n",
    "    );\n",
    "  }\n",
    "\n",
    "  const selectedStrategies = await window.showQuickPick(strategies, {\n",
    "    canPickMany: true,\n",
    "    title: \"Too many tokens\",\n",
    "    placeHolder: \"Select one or more strategies to reduce the token count\",\n",
    "  });\n",
    "\n",
    "  if (!selectedStrategies) {\n",
    "    return null;\n",
    "  }\n",
    "\n",
    "  let reducedMessages = msgs;\n",
    "  for (const strategy of selectedStrategies) {\n",
    "    reducedMessages = await strategy.apply(reducedMessages);\n",
    "  }\n",
    "\n",
    "  const reducedTokenCount = countTotalTokens(reducedMessages, model);\n",
    "  if (reducedTokenCount > limit) {\n",
    "    window.showErrorMessage(\"The selected strategies do not reduce tokens below the limit.\");\n",
    "    return null;\n",
    "  }\n",
    "\n",
    "  return reducedMessages;\n",
    "}\n",
    "\n",
    "function countTokens(text: string, model: any): number {\n",
    "  const enc = encoding_for_model(model);\n",
    "  const tokenCount = enc.encode(text).length;\n",
    "  enc.free();\n",
    "  return tokenCount;\n",
    "}\n",
    "\n",
    "function countTotalTokens(msgs: ChatCompletionRequestMessage[], model: string): number {\n",
    "  return msgs.reduce((accumulator, message) => {\n",
    "    return accumulator + countTokens(message.content, model);\n",
    "  }, 0);\n",
    "}\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "model": "gpt-4",
 "nbformat": 4,
 "nbformat_minor": 2
}
